{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc12a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "00df71d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\SML_Projects\\SML_hotelBooking_cancelling_prediction')\n",
    "os.makedirs('results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4116188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('data/preprocessed/preprocessed_x_train.csv')\n",
    "x_test = pd.read_csv('data/preprocessed/preprocessed_x_test.csv')\n",
    "\n",
    "y_train = pd.read_csv('data/split/y_train.csv')['is_canceled']\n",
    "y_test = pd.read_csv('data/split/y_test.csv')['is_canceled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a4469cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60b6eeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train class distribution: Counter({0: 60259, 1: 35253})\n",
      "After SMOTE train class distribution: Counter({1: 60259, 0: 60259})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "print(\"Original train class distribution:\", Counter(y_train))\n",
    "smote = SMOTE(random_state=42, k_neighbors=1)\n",
    "\n",
    "x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "print(\"After SMOTE train class distribution:\", Counter(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bbeb4526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.linear_model import (\n",
    "    LogisticRegression, RidgeClassifier, SGDClassifier, PassiveAggressiveClassifier\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier,\n",
    "    HistGradientBoostingClassifier, AdaBoostClassifier,\n",
    "    BaggingClassifier, VotingClassifier, StackingClassifier\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f8d4371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimators = [\n",
    "    ('rf', RandomForestClassifier(random_state=42)),\n",
    "    ('xgb', XGBClassifier(random_state=42, eval_metric='logloss')),\n",
    "    ('lgbm', LGBMClassifier(random_state=42))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a956475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(trial, name):\n",
    "\n",
    "    # ===== Linear =====\n",
    "    if name == 'Logistic Regression':\n",
    "        return LogisticRegression(\n",
    "            C=trial.suggest_float('C', 1e-2, 10.0, log=True),\n",
    "            solver=trial.suggest_categorical('solver', ['lbfgs', 'liblinear']),\n",
    "            max_iter=1000\n",
    "        )\n",
    "\n",
    "    if name == 'RidgeClassifier':\n",
    "        return RidgeClassifier(\n",
    "            alpha=trial.suggest_float('alpha', 1e-2, 10.0, log=True)\n",
    "        )\n",
    "\n",
    "    if name == 'SGDC':\n",
    "        return SGDClassifier(\n",
    "            alpha=trial.suggest_float('alpha', 1e-4, 1.0, log=True),\n",
    "            loss=trial.suggest_categorical('loss', ['log_loss', 'hinge']),\n",
    "            penalty=trial.suggest_categorical('penalty', ['l2', 'l1', 'elasticnet']),\n",
    "            l1_ratio=trial.suggest_float('l1_ratio', 0.1, 0.9),\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if name == 'Passive Aggressive':\n",
    "        return PassiveAggressiveClassifier(\n",
    "            C=trial.suggest_float('C', 1e-2, 10.0, log=True),\n",
    "            loss=trial.suggest_categorical('loss', ['hinge', 'squared_hinge']),\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    # ===== Tree =====\n",
    "    if name == 'DecisionTree':\n",
    "        return DecisionTreeClassifier(\n",
    "            max_depth=trial.suggest_int('max_depth', 2, 30),\n",
    "            min_samples_split=trial.suggest_int('min_samples_split', 2, 20),\n",
    "            min_samples_leaf=trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if name == 'RandomForest':\n",
    "        return RandomForestClassifier(\n",
    "            n_estimators=trial.suggest_int('n_estimators', 100, 600),\n",
    "            max_depth=trial.suggest_int('max_depth', 3, 30),\n",
    "            min_samples_split=trial.suggest_int('min_samples_split', 2, 10),\n",
    "            min_samples_leaf=trial.suggest_int('min_samples_leaf', 1, 4),\n",
    "            max_features=trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if name == 'ExtraTrees':\n",
    "        return ExtraTreesClassifier(\n",
    "            n_estimators=trial.suggest_int('n_estimators', 100, 600),\n",
    "            max_depth=trial.suggest_int('max_depth', 3, 30),\n",
    "            min_samples_leaf=trial.suggest_int('min_samples_leaf', 1, 5),\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if name == 'Bagging':\n",
    "        return BaggingClassifier(\n",
    "            n_estimators=trial.suggest_int('n_estimators', 50, 400),\n",
    "            max_samples=trial.suggest_float('max_samples', 0.5, 1.0),\n",
    "            max_features=trial.suggest_float('max_features', 0.5, 1.0),\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if name == 'Bagged DT':\n",
    "        return BaggingClassifier(\n",
    "            estimator=DecisionTreeClassifier(),\n",
    "            n_estimators=trial.suggest_int('n_estimators', 50, 400),\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    # ===== Boosting =====\n",
    "    if name == 'GradientBoosting':\n",
    "        return GradientBoostingClassifier(\n",
    "            n_estimators=trial.suggest_int('n_estimators', 100, 500),\n",
    "            learning_rate=trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "            max_depth=trial.suggest_int('max_depth', 2, 6),\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if name == 'HistGradientBoosting':\n",
    "        return HistGradientBoostingClassifier(\n",
    "            learning_rate=trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "            max_depth=trial.suggest_int('max_depth', 2, 10),\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if name == 'AdaBoost':\n",
    "        return AdaBoostClassifier(\n",
    "            n_estimators=trial.suggest_int('n_estimators', 100, 500),\n",
    "            learning_rate=trial.suggest_float('learning_rate', 0.01, 1.0, log=True),\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if name == 'XGBoost':\n",
    "        return XGBClassifier(\n",
    "            n_estimators=trial.suggest_int('n_estimators', 100, 500),\n",
    "            max_depth=trial.suggest_int('max_depth', 3, 10),\n",
    "            learning_rate=trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            subsample=trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            colsample_bytree=trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            eval_metric='logloss',\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if name == 'LGBMClassifier':\n",
    "        return LGBMClassifier(\n",
    "            n_estimators=trial.suggest_int('n_estimators', 100, 500),\n",
    "            learning_rate=trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            num_leaves=trial.suggest_int('num_leaves', 31, 255),\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    if name == 'CatBoost':\n",
    "        return CatBoostClassifier(\n",
    "            iterations=trial.suggest_int('iterations', 300, 800),\n",
    "            learning_rate=trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            depth=trial.suggest_int('depth', 4, 10),\n",
    "            verbose=0,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    # ===== Other =====\n",
    "    if name == 'SVC':\n",
    "        return SVC(\n",
    "            C=trial.suggest_float('C', 1e-2, 10.0, log=True),\n",
    "            kernel=trial.suggest_categorical('kernel', ['rbf', 'linear']),\n",
    "            gamma=trial.suggest_categorical('gamma', ['scale', 'auto']),\n",
    "            probability=True\n",
    "        )\n",
    "\n",
    "    if name == 'KNN':\n",
    "        return KNeighborsClassifier(\n",
    "            n_neighbors=trial.suggest_int('n_neighbors', 3, 30),\n",
    "            weights=trial.suggest_categorical('weights', ['uniform', 'distance']),\n",
    "            p=trial.suggest_int('p', 1, 2)\n",
    "        )\n",
    "\n",
    "    if name == 'MLPC':\n",
    "        return MLPClassifier(\n",
    "            hidden_layer_sizes=trial.suggest_categorical(\n",
    "                'hidden_layer_sizes', [(50,), (100,), (100, 50)]\n",
    "            ),\n",
    "            alpha=trial.suggest_float('alpha', 1e-4, 1.0, log=True),\n",
    "            learning_rate_init=trial.suggest_float('lr', 1e-4, 1e-2, log=True),\n",
    "            max_iter=500,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    # ===== Ensembles =====\n",
    "    if name == 'Hard Voting':\n",
    "        return VotingClassifier(estimators=base_estimators, voting='hard')\n",
    "\n",
    "    if name == 'Soft Voting':\n",
    "        return VotingClassifier(estimators=base_estimators, voting='soft')\n",
    "\n",
    "    if name == 'Stacking':\n",
    "        return StackingClassifier(\n",
    "            estimators=base_estimators,\n",
    "            final_estimator=LogisticRegression(max_iter=1000)\n",
    "        )\n",
    "\n",
    "    if name == 'DummyClassifier':\n",
    "        return DummyClassifier(\n",
    "            strategy=trial.suggest_categorical(\n",
    "                'strategy', ['most_frequent', 'stratified', 'uniform']\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "473bd562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, name):\n",
    "    model = get_model(trial, name)\n",
    "\n",
    "    scores = cross_val_score(\n",
    "        model,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        cv=kf,\n",
    "        scoring='recall',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    return scores.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "af9ed54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-30 19:27:48,565] A new study created in memory with name: no-name-3c009223-1b3c-42d6-bf0e-630491414b6c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸ”¥ NOW TUNING: SGDC\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-30 19:27:50,142] Trial 0 finished with value: 0.7498851901280128 and parameters: {'alpha': 0.024404106082471344, 'loss': 'hinge', 'penalty': 'l2', 'l1_ratio': 0.36824954315095737}. Best is trial 0 with value: 0.7498851901280128.\n",
      "[I 2025-12-30 19:27:51,405] Trial 1 finished with value: 0.7664589496663329 and parameters: {'alpha': 0.0007265327329036694, 'loss': 'log_loss', 'penalty': 'l1', 'l1_ratio': 0.10945511773209704}. Best is trial 1 with value: 0.7664589496663329.\n",
      "[I 2025-12-30 19:27:52,631] Trial 2 finished with value: 0.7743259185486023 and parameters: {'alpha': 0.02592430499238967, 'loss': 'log_loss', 'penalty': 'l2', 'l1_ratio': 0.4785803142229095}. Best is trial 2 with value: 0.7743259185486023.\n",
      "[I 2025-12-30 19:27:53,843] Trial 3 finished with value: 0.7336103385979067 and parameters: {'alpha': 0.006729903920130239, 'loss': 'hinge', 'penalty': 'l2', 'l1_ratio': 0.6387908465083003}. Best is trial 2 with value: 0.7743259185486023.\n",
      "[I 2025-12-30 19:27:55,098] Trial 4 finished with value: 0.6437476374700125 and parameters: {'alpha': 0.024107159806384292, 'loss': 'hinge', 'penalty': 'l1', 'l1_ratio': 0.6855865195285901}. Best is trial 2 with value: 0.7743259185486023.\n",
      "[I 2025-12-30 19:27:56,366] Trial 5 finished with value: 0.7631512894305471 and parameters: {'alpha': 0.0026716529830105563, 'loss': 'log_loss', 'penalty': 'l1', 'l1_ratio': 0.8348879630561058}. Best is trial 2 with value: 0.7743259185486023.\n",
      "[I 2025-12-30 19:27:57,528] Trial 6 finished with value: 0.706236304041718 and parameters: {'alpha': 0.00984069403587795, 'loss': 'hinge', 'penalty': 'elasticnet', 'l1_ratio': 0.5123916025614775}. Best is trial 2 with value: 0.7743259185486023.\n",
      "[I 2025-12-30 19:27:57,964] Trial 7 finished with value: 0.8488702118726122 and parameters: {'alpha': 0.5025299019596603, 'loss': 'log_loss', 'penalty': 'l2', 'l1_ratio': 0.5517049829695918}. Best is trial 7 with value: 0.8488702118726122.\n",
      "[I 2025-12-30 19:27:58,528] Trial 8 finished with value: 0.7724839812108323 and parameters: {'alpha': 0.0002845478901894153, 'loss': 'hinge', 'penalty': 'elasticnet', 'l1_ratio': 0.4683869391724097}. Best is trial 7 with value: 0.8488702118726122.\n",
      "[I 2025-12-30 19:27:58,990] Trial 9 finished with value: 0.7632512338432503 and parameters: {'alpha': 0.0010210473352983847, 'loss': 'log_loss', 'penalty': 'l2', 'l1_ratio': 0.16174868661831568}. Best is trial 7 with value: 0.8488702118726122.\n",
      "[I 2025-12-30 19:27:59,432] Trial 10 finished with value: 0.839634423746252 and parameters: {'alpha': 0.4323177978077599, 'loss': 'log_loss', 'penalty': 'l2', 'l1_ratio': 0.8936542937335118}. Best is trial 7 with value: 0.8488702118726122.\n",
      "[I 2025-12-30 19:27:59,881] Trial 11 finished with value: 0.8536443952729772 and parameters: {'alpha': 0.8488957395266886, 'loss': 'log_loss', 'penalty': 'l2', 'l1_ratio': 0.88939965455131}. Best is trial 11 with value: 0.8536443952729772.\n",
      "[I 2025-12-30 19:28:00,313] Trial 12 finished with value: 0.8492228229496822 and parameters: {'alpha': 0.8893739048701611, 'loss': 'log_loss', 'penalty': 'l2', 'l1_ratio': 0.7374995617298326}. Best is trial 11 with value: 0.8536443952729772.\n",
      "[I 2025-12-30 19:28:00,742] Trial 13 finished with value: 0.792674916796055 and parameters: {'alpha': 0.1193134889995193, 'loss': 'log_loss', 'penalty': 'l2', 'l1_ratio': 0.7694179185953914}. Best is trial 11 with value: 0.8536443952729772.\n",
      "[I 2025-12-30 19:28:01,176] Trial 14 finished with value: 0.7928074549007601 and parameters: {'alpha': 0.12119244826822871, 'loss': 'log_loss', 'penalty': 'l2', 'l1_ratio': 0.7529074310260379}. Best is trial 11 with value: 0.8536443952729772.\n",
      "[I 2025-12-30 19:28:01,649] Trial 15 finished with value: 0.6666666666666666 and parameters: {'alpha': 0.7141758169551145, 'loss': 'log_loss', 'penalty': 'elasticnet', 'l1_ratio': 0.8977959376150015}. Best is trial 11 with value: 0.8536443952729772.\n",
      "[I 2025-12-30 19:28:02,088] Trial 16 finished with value: 0.8011724620838884 and parameters: {'alpha': 0.17822806081472495, 'loss': 'log_loss', 'penalty': 'l2', 'l1_ratio': 0.6445635073035285}. Best is trial 11 with value: 0.8536443952729772.\n",
      "[I 2025-12-30 19:28:02,537] Trial 17 finished with value: 0.8384041401961445 and parameters: {'alpha': 0.9667093372435209, 'loss': 'log_loss', 'penalty': 'l2', 'l1_ratio': 0.7889336333739096}. Best is trial 11 with value: 0.8536443952729772.\n",
      "[I 2025-12-30 19:28:03,019] Trial 18 finished with value: 0.6666666666666666 and parameters: {'alpha': 0.2850991014607034, 'loss': 'log_loss', 'penalty': 'elasticnet', 'l1_ratio': 0.3474828188611559}. Best is trial 11 with value: 0.8536443952729772.\n",
      "[I 2025-12-30 19:28:03,515] Trial 19 finished with value: 0.6666666666666666 and parameters: {'alpha': 0.05908380022482488, 'loss': 'log_loss', 'penalty': 'l1', 'l1_ratio': 0.7076076934475481}. Best is trial 11 with value: 0.8536443952729772.\n",
      "[I 2025-12-30 19:28:03,963] Trial 20 finished with value: 0.8369087320460556 and parameters: {'alpha': 0.9754744243996634, 'loss': 'log_loss', 'penalty': 'l2', 'l1_ratio': 0.8322226651795329}. Best is trial 11 with value: 0.8536443952729772.\n",
      "[I 2025-12-30 19:28:04,413] Trial 21 finished with value: 0.827474707357888 and parameters: {'alpha': 0.34985163647015965, 'loss': 'log_loss', 'penalty': 'l2', 'l1_ratio': 0.5693124332435343}. Best is trial 11 with value: 0.8536443952729772.\n",
      "[I 2025-12-30 19:28:04,848] Trial 22 finished with value: 0.7865693009798661 and parameters: {'alpha': 0.07577010855658271, 'loss': 'log_loss', 'penalty': 'l2', 'l1_ratio': 0.5543298310552119}. Best is trial 11 with value: 0.8536443952729772.\n",
      "[I 2025-12-30 19:28:05,321] Trial 23 finished with value: 0.8371798219745781 and parameters: {'alpha': 0.41334920238764034, 'loss': 'log_loss', 'penalty': 'l2', 'l1_ratio': 0.3990133927952914}. Best is trial 11 with value: 0.8536443952729772.\n",
      "[I 2025-12-30 19:28:05,766] Trial 24 finished with value: 0.8123248256248564 and parameters: {'alpha': 0.255490010938724, 'loss': 'log_loss', 'penalty': 'l2', 'l1_ratio': 0.26436204752287323}. Best is trial 11 with value: 0.8536443952729772.\n",
      "[I 2025-12-30 19:28:06,282] Trial 25 finished with value: 0.768201451427669 and parameters: {'alpha': 0.00011722457457519848, 'loss': 'log_loss', 'penalty': 'l2', 'l1_ratio': 0.6339828793080007}. Best is trial 11 with value: 0.8536443952729772.\n",
      "[I 2025-12-30 19:28:06,723] Trial 26 finished with value: 0.7432703963865226 and parameters: {'alpha': 0.6674280604818348, 'loss': 'hinge', 'penalty': 'l2', 'l1_ratio': 0.8351476827698664}. Best is trial 11 with value: 0.8536443952729772.\n",
      "[I 2025-12-30 19:28:07,191] Trial 27 finished with value: 0.7845018696003274 and parameters: {'alpha': 0.0494360776273828, 'loss': 'log_loss', 'penalty': 'l2', 'l1_ratio': 0.5758791188068533}. Best is trial 11 with value: 0.8536443952729772.\n",
      "[I 2025-12-30 19:28:07,673] Trial 28 finished with value: 0.6666666666666666 and parameters: {'alpha': 0.1434916300443563, 'loss': 'log_loss', 'penalty': 'l1', 'l1_ratio': 0.7184317314047856}. Best is trial 11 with value: 0.8536443952729772.\n",
      "[I 2025-12-30 19:28:08,148] Trial 29 finished with value: 1.0 and parameters: {'alpha': 0.5412011829901321, 'loss': 'hinge', 'penalty': 'elasticnet', 'l1_ratio': 0.39819322724543377}. Best is trial 29 with value: 1.0.\n",
      "[I 2025-12-30 19:28:08,832] A new study created in memory with name: no-name-8c31bf93-4f04-4f48-bb25-768cb57cd363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULTS FOR: SGDC\n",
      "----------------------------------------\n",
      "ACCURACY     : 0.3757\n",
      "PRECISION    : 0.3757\n",
      "RECALL       : 1.0000\n",
      "F1 SCORE     : 0.5462\n",
      "K-FOLD MEAN  : 0.3333\n",
      "K-FOLD STD   : 0.0003\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "ðŸ”¥ NOW TUNING: Bagging\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-30 19:31:04,391] Trial 0 finished with value: 0.890491113861947 and parameters: {'n_estimators': 391, 'max_samples': 0.6200956159564395, 'max_features': 0.9353368380701383}. Best is trial 0 with value: 0.890491113861947.\n",
      "[I 2025-12-30 19:33:22,679] Trial 1 finished with value: 0.8926978913720337 and parameters: {'n_estimators': 321, 'max_samples': 0.8151229664071127, 'max_features': 0.838402240397144}. Best is trial 1 with value: 0.8926978913720337.\n",
      "[I 2025-12-30 19:34:13,380] Trial 2 finished with value: 0.8882505000599838 and parameters: {'n_estimators': 159, 'max_samples': 0.5232294738940722, 'max_features': 0.8425505563568015}. Best is trial 1 with value: 0.8926978913720337.\n",
      "[I 2025-12-30 19:35:25,808] Trial 3 finished with value: 0.8928467773364913 and parameters: {'n_estimators': 147, 'max_samples': 0.8767186429361205, 'max_features': 0.9174245110995658}. Best is trial 3 with value: 0.8928467773364913.\n",
      "[I 2025-12-30 19:36:38,459] Trial 4 finished with value: 0.8912372769840536 and parameters: {'n_estimators': 219, 'max_samples': 0.6680014710672775, 'max_features': 0.7324728426070289}. Best is trial 3 with value: 0.8928467773364913.\n",
      "[I 2025-12-30 19:37:26,645] Trial 5 finished with value: 0.8906733444438425 and parameters: {'n_estimators': 131, 'max_samples': 0.8110516615263282, 'max_features': 0.718757928556043}. Best is trial 3 with value: 0.8928467773364913.\n",
      "[I 2025-12-30 19:38:01,514] Trial 6 finished with value: 0.889229054743606 and parameters: {'n_estimators': 76, 'max_samples': 0.880799014715328, 'max_features': 0.8207972885374248}. Best is trial 3 with value: 0.8928467773364913.\n",
      "[I 2025-12-30 19:39:21,621] Trial 7 finished with value: 0.8843671007613497 and parameters: {'n_estimators': 362, 'max_samples': 0.5134419086823347, 'max_features': 0.5990927782710989}. Best is trial 3 with value: 0.8928467773364913.\n",
      "[I 2025-12-30 19:40:51,752] Trial 8 finished with value: 0.890274207949958 and parameters: {'n_estimators': 277, 'max_samples': 0.8085655151278339, 'max_features': 0.6364807896861224}. Best is trial 3 with value: 0.8928467773364913.\n",
      "[I 2025-12-30 19:42:06,109] Trial 9 finished with value: 0.8864584263097331 and parameters: {'n_estimators': 283, 'max_samples': 0.6268046949724134, 'max_features': 0.6031998857876115}. Best is trial 3 with value: 0.8928467773364913.\n",
      "[I 2025-12-30 19:44:00,822] Trial 10 finished with value: 0.8927478022237713 and parameters: {'n_estimators': 195, 'max_samples': 0.9950905425401476, 'max_features': 0.9945710298006859}. Best is trial 3 with value: 0.8928467773364913.\n",
      "[I 2025-12-30 19:45:54,979] Trial 11 finished with value: 0.8928150717876547 and parameters: {'n_estimators': 199, 'max_samples': 0.9999156517585602, 'max_features': 0.9633054374567455}. Best is trial 3 with value: 0.8928467773364913.\n",
      "[I 2025-12-30 19:46:46,066] Trial 12 finished with value: 0.8920513275453889 and parameters: {'n_estimators': 93, 'max_samples': 0.9941753833237696, 'max_features': 0.9365458098265168}. Best is trial 3 with value: 0.8928467773364913.\n",
      "[I 2025-12-30 19:48:14,857] Trial 13 finished with value: 0.8906565867424295 and parameters: {'n_estimators': 158, 'max_samples': 0.909176203197145, 'max_features': 0.9940353899174258}. Best is trial 3 with value: 0.8928467773364913.\n",
      "[I 2025-12-30 19:50:22,397] Trial 14 finished with value: 0.8939100935351932 and parameters: {'n_estimators': 249, 'max_samples': 0.9218619817207638, 'max_features': 0.9019717328514377}. Best is trial 14 with value: 0.8939100935351932.\n",
      "[I 2025-12-30 19:52:30,523] Trial 15 finished with value: 0.8932129379891887 and parameters: {'n_estimators': 259, 'max_samples': 0.9084850586730806, 'max_features': 0.8885914250217857}. Best is trial 14 with value: 0.8939100935351932.\n",
      "[I 2025-12-30 19:54:18,440] Trial 16 finished with value: 0.8911389509856201 and parameters: {'n_estimators': 258, 'max_samples': 0.7298151671962959, 'max_features': 0.8745124021229803}. Best is trial 14 with value: 0.8939100935351932.\n",
      "[I 2025-12-30 19:56:39,944] Trial 17 finished with value: 0.8939094340800441 and parameters: {'n_estimators': 319, 'max_samples': 0.923311212491835, 'max_features': 0.7874126711302634}. Best is trial 14 with value: 0.8939100935351932.\n",
      "[I 2025-12-30 19:59:06,637] Trial 18 finished with value: 0.8932124087684122 and parameters: {'n_estimators': 322, 'max_samples': 0.9528419146663611, 'max_features': 0.7986364976879374}. Best is trial 14 with value: 0.8939100935351932.\n",
      "[I 2025-12-30 20:00:25,872] Trial 19 finished with value: 0.8827406568170347 and parameters: {'n_estimators': 322, 'max_samples': 0.7395264612008912, 'max_features': 0.5165059654240771}. Best is trial 14 with value: 0.8939100935351932.\n",
      "[I 2025-12-30 20:02:35,583] Trial 20 finished with value: 0.8915696031044105 and parameters: {'n_estimators': 359, 'max_samples': 0.8348134962573814, 'max_features': 0.6800271289042605}. Best is trial 14 with value: 0.8939100935351932.\n",
      "[I 2025-12-30 20:04:22,714] Trial 21 finished with value: 0.8930963041733507 and parameters: {'n_estimators': 248, 'max_samples': 0.9253166442556695, 'max_features': 0.7799742745117071}. Best is trial 14 with value: 0.8939100935351932.\n",
      "[I 2025-12-30 20:06:45,684] Trial 22 finished with value: 0.8924156735798832 and parameters: {'n_estimators': 297, 'max_samples': 0.8643257343725099, 'max_features': 0.8821431143887709}. Best is trial 14 with value: 0.8939100935351932.\n",
      "[I 2025-12-30 20:08:42,974] Trial 23 finished with value: 0.8938100478805217 and parameters: {'n_estimators': 233, 'max_samples': 0.9380701333229261, 'max_features': 0.8861622835682509}. Best is trial 14 with value: 0.8939100935351932.\n",
      "[I 2025-12-30 20:10:25,794] Trial 24 finished with value: 0.8931781712162018 and parameters: {'n_estimators': 227, 'max_samples': 0.9547068586172471, 'max_features': 0.7882256613218019}. Best is trial 14 with value: 0.8939100935351932.\n",
      "[I 2025-12-30 20:11:40,012] Trial 25 finished with value: 0.8912217797485228 and parameters: {'n_estimators': 196, 'max_samples': 0.7759468606850579, 'max_features': 0.7594054228207217}. Best is trial 14 with value: 0.8939100935351932.\n",
      "[I 2025-12-30 20:14:09,045] Trial 26 finished with value: 0.8943239219556162 and parameters: {'n_estimators': 301, 'max_samples': 0.9483761692089484, 'max_features': 0.8633394375861141}. Best is trial 26 with value: 0.8943239219556162.\n",
      "[I 2025-12-30 20:16:49,743] Trial 27 finished with value: 0.8917515054610702 and parameters: {'n_estimators': 352, 'max_samples': 0.8547481250062066, 'max_features': 0.8493140407489423}. Best is trial 26 with value: 0.8943239219556162.\n",
      "[I 2025-12-30 20:18:47,253] Trial 28 finished with value: 0.8915531620422289 and parameters: {'n_estimators': 302, 'max_samples': 0.8952640820278901, 'max_features': 0.7072580363598192}. Best is trial 26 with value: 0.8943239219556162.\n",
      "[I 2025-12-30 20:22:13,040] Trial 29 finished with value: 0.8927145785286319 and parameters: {'n_estimators': 380, 'max_samples': 0.9542598590401481, 'max_features': 0.9364812023382562}. Best is trial 26 with value: 0.8943239219556162.\n",
      "[I 2025-12-30 20:28:30,445] A new study created in memory with name: no-name-e287cb3b-e314-464e-8160-44302bc862c9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULTS FOR: Bagging\n",
      "----------------------------------------\n",
      "ACCURACY     : 0.8856\n",
      "PRECISION    : 0.8594\n",
      "RECALL       : 0.8317\n",
      "F1 SCORE     : 0.8453\n",
      "K-FOLD MEAN  : 0.9026\n",
      "K-FOLD STD   : 0.0007\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "ðŸ”¥ NOW TUNING: Stacking\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-30 20:29:29,003] Trial 0 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:30:27,668] Trial 1 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:31:25,955] Trial 2 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:32:23,590] Trial 3 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:33:19,102] Trial 4 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:34:14,485] Trial 5 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:35:09,954] Trial 6 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:36:07,357] Trial 7 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:37:05,070] Trial 8 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:38:00,599] Trial 9 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:38:56,284] Trial 10 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:39:51,659] Trial 11 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:40:47,520] Trial 12 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:41:43,628] Trial 13 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:42:39,852] Trial 14 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:43:35,951] Trial 15 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:44:32,254] Trial 16 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:45:28,205] Trial 17 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:46:24,415] Trial 18 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:47:20,315] Trial 19 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:48:16,376] Trial 20 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:49:12,660] Trial 21 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:50:09,550] Trial 22 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:51:05,769] Trial 23 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:52:02,114] Trial 24 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:52:58,212] Trial 25 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:53:54,396] Trial 26 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:54:50,366] Trial 27 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:55:47,045] Trial 28 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n",
      "[I 2025-12-30 20:56:43,401] Trial 29 finished with value: 0.9002984549028138 and parameters: {}. Best is trial 0 with value: 0.9002984549028138.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 60259, number of negative: 60259\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005789 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7341\n",
      "[LightGBM] [Info] Number of data points in the train set: 120518, number of used features: 49\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 48207, number of negative: 48207\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004789 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7365\n",
      "[LightGBM] [Info] Number of data points in the train set: 96414, number of used features: 49\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 48207, number of negative: 48207\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004766 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7364\n",
      "[LightGBM] [Info] Number of data points in the train set: 96414, number of used features: 49\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 48207, number of negative: 48207\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005049 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7349\n",
      "[LightGBM] [Info] Number of data points in the train set: 96414, number of used features: 49\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 48207, number of negative: 48208\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004969 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7062\n",
      "[LightGBM] [Info] Number of data points in the train set: 96415, number of used features: 49\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499995 -> initscore=-0.000021\n",
      "[LightGBM] [Info] Start training from score -0.000021\n",
      "[LightGBM] [Info] Number of positive: 48208, number of negative: 48207\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005217 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7106\n",
      "[LightGBM] [Info] Number of data points in the train set: 96415, number of used features: 49\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500005 -> initscore=0.000021\n",
      "[LightGBM] [Info] Start training from score 0.000021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-30 20:58:57,183] A new study created in memory with name: no-name-b3a56e4f-10ae-45f6-a2f3-b3b368a05160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULTS FOR: Stacking\n",
      "----------------------------------------\n",
      "ACCURACY     : 0.8868\n",
      "PRECISION    : 0.8599\n",
      "RECALL       : 0.8346\n",
      "F1 SCORE     : 0.8470\n",
      "K-FOLD MEAN  : 0.9046\n",
      "K-FOLD STD   : 0.0012\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "ðŸ”¥ NOW TUNING: LGBMClassifier\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-30 20:58:58,351] Trial 0 finished with value: 0.8866904680171871 and parameters: {'n_estimators': 115, 'learning_rate': 0.16627348074994858, 'num_leaves': 53}. Best is trial 0 with value: 0.8866904680171871.\n",
      "[I 2025-12-30 20:59:03,180] Trial 1 finished with value: 0.8801197798605639 and parameters: {'n_estimators': 282, 'learning_rate': 0.015539779533567357, 'num_leaves': 147}. Best is trial 0 with value: 0.8866904680171871.\n",
      "[I 2025-12-30 20:59:07,417] Trial 2 finished with value: 0.8776310815446213 and parameters: {'n_estimators': 416, 'learning_rate': 0.013714283634056433, 'num_leaves': 73}. Best is trial 0 with value: 0.8866904680171871.\n",
      "[I 2025-12-30 20:59:15,705] Trial 3 finished with value: 0.8935106240518592 and parameters: {'n_estimators': 272, 'learning_rate': 0.1224838143403789, 'num_leaves': 248}. Best is trial 3 with value: 0.8935106240518592.\n",
      "[I 2025-12-30 20:59:25,214] Trial 4 finished with value: 0.8867739748593869 and parameters: {'n_estimators': 350, 'learning_rate': 0.01740237584934604, 'num_leaves': 186}. Best is trial 3 with value: 0.8935106240518592.\n",
      "[I 2025-12-30 20:59:29,631] Trial 5 finished with value: 0.8726525806415598 and parameters: {'n_estimators': 184, 'learning_rate': 0.011645824523679923, 'num_leaves': 221}. Best is trial 3 with value: 0.8935106240518592.\n",
      "[I 2025-12-30 20:59:33,306] Trial 6 finished with value: 0.8768673587148282 and parameters: {'n_estimators': 266, 'learning_rate': 0.03871084480899581, 'num_leaves': 33}. Best is trial 3 with value: 0.8935106240518592.\n",
      "[I 2025-12-30 20:59:38,089] Trial 7 finished with value: 0.8925151262907539 and parameters: {'n_estimators': 278, 'learning_rate': 0.06674885981191728, 'num_leaves': 182}. Best is trial 3 with value: 0.8935106240518592.\n",
      "[I 2025-12-30 20:59:41,249] Trial 8 finished with value: 0.8905905081284017 and parameters: {'n_estimators': 375, 'learning_rate': 0.0732419932645865, 'num_leaves': 71}. Best is trial 3 with value: 0.8935106240518592.\n",
      "[I 2025-12-30 20:59:45,742] Trial 9 finished with value: 0.8924665449065307 and parameters: {'n_estimators': 232, 'learning_rate': 0.1904388640783408, 'num_leaves': 206}. Best is trial 3 with value: 0.8935106240518592.\n",
      "[I 2025-12-30 20:59:57,682] Trial 10 finished with value: 0.8926472938596722 and parameters: {'n_estimators': 497, 'learning_rate': 0.2969795621852368, 'num_leaves': 254}. Best is trial 3 with value: 0.8935106240518592.\n",
      "[I 2025-12-30 21:00:08,962] Trial 11 finished with value: 0.8940067457070121 and parameters: {'n_estimators': 494, 'learning_rate': 0.25355446546613863, 'num_leaves': 249}. Best is trial 11 with value: 0.8940067457070121.\n",
      "[I 2025-12-30 21:00:20,263] Trial 12 finished with value: 0.8932106485611593 and parameters: {'n_estimators': 476, 'learning_rate': 0.1207193732216601, 'num_leaves': 254}. Best is trial 11 with value: 0.8940067457070121.\n",
      "[I 2025-12-30 21:00:24,603] Trial 13 finished with value: 0.8904729108329762 and parameters: {'n_estimators': 347, 'learning_rate': 0.2990048435279827, 'num_leaves': 127}. Best is trial 11 with value: 0.8940067457070121.\n",
      "[I 2025-12-30 21:00:33,066] Trial 14 finished with value: 0.8947208720503687 and parameters: {'n_estimators': 424, 'learning_rate': 0.10801890969354885, 'num_leaves': 238}. Best is trial 14 with value: 0.8947208720503687.\n",
      "[I 2025-12-30 21:00:41,941] Trial 15 finished with value: 0.8946881324771297 and parameters: {'n_estimators': 445, 'learning_rate': 0.04365805098357711, 'num_leaves': 221}. Best is trial 14 with value: 0.8947208720503687.\n",
      "[I 2025-12-30 21:00:48,537] Trial 16 finished with value: 0.891503477631971 and parameters: {'n_estimators': 430, 'learning_rate': 0.03153837235779033, 'num_leaves': 147}. Best is trial 14 with value: 0.8947208720503687.\n",
      "[I 2025-12-30 21:00:57,874] Trial 17 finished with value: 0.8942743249547854 and parameters: {'n_estimators': 432, 'learning_rate': 0.046199431510221815, 'num_leaves': 216}. Best is trial 14 with value: 0.8947208720503687.\n",
      "[I 2025-12-30 21:01:04,797] Trial 18 finished with value: 0.8913869758100416 and parameters: {'n_estimators': 384, 'learning_rate': 0.02952417083631984, 'num_leaves': 174}. Best is trial 14 with value: 0.8947208720503687.\n",
      "[I 2025-12-30 21:01:12,920] Trial 19 finished with value: 0.8942560977250172 and parameters: {'n_estimators': 445, 'learning_rate': 0.10128353280106804, 'num_leaves': 219}. Best is trial 14 with value: 0.8947208720503687.\n",
      "[I 2025-12-30 21:01:17,389] Trial 20 finished with value: 0.8857458891718885 and parameters: {'n_estimators': 336, 'learning_rate': 0.02313439760800014, 'num_leaves': 111}. Best is trial 14 with value: 0.8947208720503687.\n",
      "[I 2025-12-30 21:01:26,095] Trial 21 finished with value: 0.8938582899678994 and parameters: {'n_estimators': 453, 'learning_rate': 0.04866302005297418, 'num_leaves': 222}. Best is trial 14 with value: 0.8947208720503687.\n",
      "[I 2025-12-30 21:01:33,714] Trial 22 finished with value: 0.8933112188887464 and parameters: {'n_estimators': 407, 'learning_rate': 0.05261238873310426, 'num_leaves': 199}. Best is trial 14 with value: 0.8947208720503687.\n",
      "[I 2025-12-30 21:01:43,061] Trial 23 finished with value: 0.8948377821707759 and parameters: {'n_estimators': 452, 'learning_rate': 0.0781057576209553, 'num_leaves': 234}. Best is trial 23 with value: 0.8948377821707759.\n",
      "[I 2025-12-30 21:01:52,525] Trial 24 finished with value: 0.8939582870046229 and parameters: {'n_estimators': 463, 'learning_rate': 0.08463816406109674, 'num_leaves': 239}. Best is trial 23 with value: 0.8948377821707759.\n",
      "[I 2025-12-30 21:01:58,763] Trial 25 finished with value: 0.8932117729996661 and parameters: {'n_estimators': 389, 'learning_rate': 0.06531208565465561, 'num_leaves': 165}. Best is trial 23 with value: 0.8948377821707759.\n",
      "[I 2025-12-30 21:02:07,908] Trial 26 finished with value: 0.8937245458082623 and parameters: {'n_estimators': 470, 'learning_rate': 0.1566143493886362, 'num_leaves': 236}. Best is trial 23 with value: 0.8948377821707759.\n",
      "[I 2025-12-30 21:02:14,233] Trial 27 finished with value: 0.8936764143023205 and parameters: {'n_estimators': 330, 'learning_rate': 0.09414650648172032, 'num_leaves': 197}. Best is trial 23 with value: 0.8948377821707759.\n",
      "[I 2025-12-30 21:02:22,704] Trial 28 finished with value: 0.8933940232344985 and parameters: {'n_estimators': 397, 'learning_rate': 0.0333313352934094, 'num_leaves': 229}. Best is trial 23 with value: 0.8948377821707759.\n",
      "[I 2025-12-30 21:02:25,652] Trial 29 finished with value: 0.8921821656467929 and parameters: {'n_estimators': 157, 'learning_rate': 0.14102431400910045, 'num_leaves': 165}. Best is trial 23 with value: 0.8948377821707759.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 60259, number of negative: 60259\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012512 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7341\n",
      "[LightGBM] [Info] Number of data points in the train set: 120518, number of used features: 49\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-30 21:02:38,006] A new study created in memory with name: no-name-aad2e097-b73d-4146-ab89-9f9dd0450b9b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULTS FOR: LGBMClassifier\n",
      "----------------------------------------\n",
      "ACCURACY     : 0.8826\n",
      "PRECISION    : 0.8561\n",
      "RECALL       : 0.8264\n",
      "F1 SCORE     : 0.8410\n",
      "K-FOLD MEAN  : 0.9045\n",
      "K-FOLD STD   : 0.0009\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "ðŸ”¥ NOW TUNING: XGBoost\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-30 21:02:40,685] Trial 0 finished with value: 0.8672241192907727 and parameters: {'n_estimators': 341, 'max_depth': 3, 'learning_rate': 0.10399674525606158, 'subsample': 0.9874634920623881, 'colsample_bytree': 0.6619299594053217}. Best is trial 0 with value: 0.8672241192907727.\n",
      "[I 2025-12-30 21:02:43,291] Trial 1 finished with value: 0.8400408019331503 and parameters: {'n_estimators': 215, 'max_depth': 7, 'learning_rate': 0.017532662344746333, 'subsample': 0.8993556216056748, 'colsample_bytree': 0.8104953480582389}. Best is trial 0 with value: 0.8672241192907727.\n",
      "[I 2025-12-30 21:02:45,984] Trial 2 finished with value: 0.8253748552940041 and parameters: {'n_estimators': 153, 'max_depth': 8, 'learning_rate': 0.010574593038517103, 'subsample': 0.7702027026406532, 'colsample_bytree': 0.7471165338516502}. Best is trial 0 with value: 0.8672241192907727.\n",
      "[I 2025-12-30 21:02:49,289] Trial 3 finished with value: 0.8277134517640888 and parameters: {'n_estimators': 156, 'max_depth': 7, 'learning_rate': 0.013118393261970958, 'subsample': 0.6914680741093906, 'colsample_bytree': 0.6294283998215906}. Best is trial 0 with value: 0.8672241192907727.\n",
      "[I 2025-12-30 21:02:51,778] Trial 4 finished with value: 0.8367079694129842 and parameters: {'n_estimators': 128, 'max_depth': 6, 'learning_rate': 0.03490924289880059, 'subsample': 0.643242028185827, 'colsample_bytree': 0.7583026674150168}. Best is trial 0 with value: 0.8672241192907727.\n",
      "[I 2025-12-30 21:02:58,753] Trial 5 finished with value: 0.872353495595349 and parameters: {'n_estimators': 264, 'max_depth': 10, 'learning_rate': 0.019277576168112896, 'subsample': 0.8423687757249879, 'colsample_bytree': 0.9851068619901718}. Best is trial 5 with value: 0.872353495595349.\n",
      "[I 2025-12-30 21:03:08,804] Trial 6 finished with value: 0.8910542091234772 and parameters: {'n_estimators': 465, 'max_depth': 10, 'learning_rate': 0.05995373706401293, 'subsample': 0.7226882479322825, 'colsample_bytree': 0.9834336623857409}. Best is trial 6 with value: 0.8910542091234772.\n",
      "[I 2025-12-30 21:03:10,785] Trial 7 finished with value: 0.8370231121544437 and parameters: {'n_estimators': 116, 'max_depth': 8, 'learning_rate': 0.02101921036312639, 'subsample': 0.8096130164230797, 'colsample_bytree': 0.8032583269739618}. Best is trial 6 with value: 0.8910542091234772.\n",
      "[I 2025-12-30 21:03:13,717] Trial 8 finished with value: 0.8284619189049071 and parameters: {'n_estimators': 397, 'max_depth': 3, 'learning_rate': 0.02111688806280041, 'subsample': 0.8149537552550524, 'colsample_bytree': 0.7881220089479724}. Best is trial 6 with value: 0.8910542091234772.\n",
      "[I 2025-12-30 21:03:17,512] Trial 9 finished with value: 0.8844163856708097 and parameters: {'n_estimators': 233, 'max_depth': 10, 'learning_rate': 0.043732749125371465, 'subsample': 0.8636268416398049, 'colsample_bytree': 0.8137472761389147}. Best is trial 6 with value: 0.8910542091234772.\n",
      "[I 2025-12-30 21:03:21,976] Trial 10 finished with value: 0.8851801271247511 and parameters: {'n_estimators': 484, 'max_depth': 5, 'learning_rate': 0.28975417871333464, 'subsample': 0.7303070905440329, 'colsample_bytree': 0.9954280727200022}. Best is trial 6 with value: 0.8910542091234772.\n",
      "[I 2025-12-30 21:03:26,484] Trial 11 finished with value: 0.885046085248072 and parameters: {'n_estimators': 499, 'max_depth': 5, 'learning_rate': 0.29344210491720424, 'subsample': 0.725944015089297, 'colsample_bytree': 0.9861560177359008}. Best is trial 6 with value: 0.8910542091234772.\n",
      "[I 2025-12-30 21:03:31,059] Trial 12 finished with value: 0.8850963925959298 and parameters: {'n_estimators': 496, 'max_depth': 5, 'learning_rate': 0.11860111916978132, 'subsample': 0.6076394136193258, 'colsample_bytree': 0.9035371612210114}. Best is trial 6 with value: 0.8910542091234772.\n",
      "[I 2025-12-30 21:03:36,019] Trial 13 finished with value: 0.8860242885344496 and parameters: {'n_estimators': 418, 'max_depth': 5, 'learning_rate': 0.2889974767318166, 'subsample': 0.7134170943890331, 'colsample_bytree': 0.9074940224849932}. Best is trial 6 with value: 0.8910542091234772.\n",
      "[I 2025-12-30 21:03:42,810] Trial 14 finished with value: 0.8883318812213982 and parameters: {'n_estimators': 418, 'max_depth': 9, 'learning_rate': 0.10363305781028583, 'subsample': 0.6839758201151853, 'colsample_bytree': 0.9094741916643713}. Best is trial 6 with value: 0.8910542091234772.\n",
      "[I 2025-12-30 21:03:49,160] Trial 15 finished with value: 0.8897591457133326 and parameters: {'n_estimators': 415, 'max_depth': 9, 'learning_rate': 0.0841425104617024, 'subsample': 0.6523778033214842, 'colsample_bytree': 0.9074932306399629}. Best is trial 6 with value: 0.8910542091234772.\n",
      "[I 2025-12-30 21:03:54,444] Trial 16 finished with value: 0.888897013509145 and parameters: {'n_estimators': 350, 'max_depth': 9, 'learning_rate': 0.07215002945853202, 'subsample': 0.6388051012453969, 'colsample_bytree': 0.8639615810905892}. Best is trial 6 with value: 0.8910542091234772.\n",
      "[I 2025-12-30 21:04:01,166] Trial 17 finished with value: 0.8899754153421813 and parameters: {'n_estimators': 456, 'max_depth': 9, 'learning_rate': 0.06925583147107421, 'subsample': 0.7791128797358124, 'colsample_bytree': 0.9506353565859731}. Best is trial 6 with value: 0.8910542091234772.\n",
      "[I 2025-12-30 21:04:09,327] Trial 18 finished with value: 0.8911371484678158 and parameters: {'n_estimators': 456, 'max_depth': 10, 'learning_rate': 0.053516493439880776, 'subsample': 0.766407321167877, 'colsample_bytree': 0.9449690663115216}. Best is trial 18 with value: 0.8911371484678158.\n",
      "[I 2025-12-30 21:04:14,797] Trial 19 finished with value: 0.8852305589139794 and parameters: {'n_estimators': 334, 'max_depth': 10, 'learning_rate': 0.031747809452740874, 'subsample': 0.7489523435861354, 'colsample_bytree': 0.8620756472439642}. Best is trial 18 with value: 0.8911371484678158.\n",
      "[I 2025-12-30 21:04:19,802] Trial 20 finished with value: 0.8901237466399233 and parameters: {'n_estimators': 379, 'max_depth': 8, 'learning_rate': 0.16718093087900904, 'subsample': 0.9368175257876885, 'colsample_bytree': 0.9385230535663882}. Best is trial 18 with value: 0.8911371484678158.\n",
      "[I 2025-12-30 21:04:24,894] Trial 21 finished with value: 0.8905724690630284 and parameters: {'n_estimators': 379, 'max_depth': 8, 'learning_rate': 0.16493003282067126, 'subsample': 0.9470642868282039, 'colsample_bytree': 0.9465153262363768}. Best is trial 18 with value: 0.8911371484678158.\n",
      "[I 2025-12-30 21:04:32,774] Trial 22 finished with value: 0.8918682022691847 and parameters: {'n_estimators': 438, 'max_depth': 10, 'learning_rate': 0.16781033701705292, 'subsample': 0.9326766489849913, 'colsample_bytree': 0.9504744729285267}. Best is trial 22 with value: 0.8918682022691847.\n",
      "[I 2025-12-30 21:04:40,136] Trial 23 finished with value: 0.8913359747634159 and parameters: {'n_estimators': 450, 'max_depth': 10, 'learning_rate': 0.055695091428960716, 'subsample': 0.8755830371225283, 'colsample_bytree': 0.8650058092952175}. Best is trial 22 with value: 0.8918682022691847.\n",
      "[I 2025-12-30 21:04:47,166] Trial 24 finished with value: 0.8892452739121683 and parameters: {'n_estimators': 446, 'max_depth': 10, 'learning_rate': 0.03900823624209463, 'subsample': 0.8774686459668257, 'colsample_bytree': 0.8674067742380187}. Best is trial 22 with value: 0.8918682022691847.\n",
      "[I 2025-12-30 21:04:51,416] Trial 25 finished with value: 0.8863084610056443 and parameters: {'n_estimators': 301, 'max_depth': 9, 'learning_rate': 0.050368170270106954, 'subsample': 0.9165004430514219, 'colsample_bytree': 0.8466582104448959}. Best is trial 22 with value: 0.8918682022691847.\n",
      "[I 2025-12-30 21:04:59,993] Trial 26 finished with value: 0.8909698529021978 and parameters: {'n_estimators': 448, 'max_depth': 10, 'learning_rate': 0.1911527434439083, 'subsample': 0.9629150758252888, 'colsample_bytree': 0.9465494141884461}. Best is trial 22 with value: 0.8918682022691847.\n",
      "[I 2025-12-30 21:05:05,790] Trial 27 finished with value: 0.8845336268047769 and parameters: {'n_estimators': 430, 'max_depth': 9, 'learning_rate': 0.030108000245122692, 'subsample': 0.8393299910722903, 'colsample_bytree': 0.8836521346204448}. Best is trial 22 with value: 0.8918682022691847.\n",
      "[I 2025-12-30 21:05:12,384] Trial 28 finished with value: 0.8909705720469047 and parameters: {'n_estimators': 370, 'max_depth': 10, 'learning_rate': 0.12552169121189888, 'subsample': 0.8914636260365323, 'colsample_bytree': 0.8416664124089924}. Best is trial 22 with value: 0.8918682022691847.\n",
      "[I 2025-12-30 21:05:14,831] Trial 29 finished with value: 0.8629925076926689 and parameters: {'n_estimators': 320, 'max_depth': 3, 'learning_rate': 0.08467998757371589, 'subsample': 0.921832312032041, 'colsample_bytree': 0.6979265241724242}. Best is trial 22 with value: 0.8918682022691847.\n",
      "[I 2025-12-30 21:05:24,607] A new study created in memory with name: no-name-8a748f34-ce38-43aa-86b9-724c9febf4e4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULTS FOR: XGBoost\n",
      "----------------------------------------\n",
      "ACCURACY     : 0.8817\n",
      "PRECISION    : 0.8578\n",
      "RECALL       : 0.8213\n",
      "F1 SCORE     : 0.8392\n",
      "K-FOLD MEAN  : 0.9009\n",
      "K-FOLD STD   : 0.0019\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "ðŸ”¥ NOW TUNING: CatBoost\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-30 21:05:31,010] Trial 0 finished with value: 0.8583966918804475 and parameters: {'iterations': 421, 'learning_rate': 0.01864630912254999, 'depth': 7}. Best is trial 0 with value: 0.8583966918804475.\n",
      "[I 2025-12-30 21:05:40,356] Trial 1 finished with value: 0.8832714067275161 and parameters: {'iterations': 766, 'learning_rate': 0.07863728928422185, 'depth': 6}. Best is trial 1 with value: 0.8832714067275161.\n",
      "[I 2025-12-30 21:05:51,673] Trial 2 finished with value: 0.8852469385270672 and parameters: {'iterations': 765, 'learning_rate': 0.0709841098365203, 'depth': 7}. Best is trial 2 with value: 0.8852469385270672.\n",
      "[I 2025-12-30 21:05:58,226] Trial 3 finished with value: 0.874492487151922 and parameters: {'iterations': 612, 'learning_rate': 0.04560542352364795, 'depth': 5}. Best is trial 2 with value: 0.8852469385270672.\n",
      "[I 2025-12-30 21:06:04,288] Trial 4 finished with value: 0.8835196869584284 and parameters: {'iterations': 646, 'learning_rate': 0.22329230767766617, 'depth': 4}. Best is trial 2 with value: 0.8852469385270672.\n",
      "[I 2025-12-30 21:06:09,961] Trial 5 finished with value: 0.882225644957893 and parameters: {'iterations': 526, 'learning_rate': 0.14649433667242634, 'depth': 5}. Best is trial 2 with value: 0.8852469385270672.\n",
      "[I 2025-12-30 21:06:16,608] Trial 6 finished with value: 0.8431125592612414 and parameters: {'iterations': 714, 'learning_rate': 0.01068541283021345, 'depth': 4}. Best is trial 2 with value: 0.8852469385270672.\n",
      "[I 2025-12-30 21:06:27,046] Trial 7 finished with value: 0.85721876958005 and parameters: {'iterations': 340, 'learning_rate': 0.014859335520949549, 'depth': 9}. Best is trial 2 with value: 0.8852469385270672.\n",
      "[I 2025-12-30 21:06:33,989] Trial 8 finished with value: 0.8770488036406681 and parameters: {'iterations': 665, 'learning_rate': 0.04871175111565272, 'depth': 5}. Best is trial 2 with value: 0.8852469385270672.\n",
      "[I 2025-12-30 21:07:01,946] Trial 9 finished with value: 0.8875513195881615 and parameters: {'iterations': 468, 'learning_rate': 0.21796820964597893, 'depth': 10}. Best is trial 9 with value: 0.8875513195881615.\n",
      "[I 2025-12-30 21:07:30,713] Trial 10 finished with value: 0.8865233913405106 and parameters: {'iterations': 479, 'learning_rate': 0.2553425725319171, 'depth': 10}. Best is trial 9 with value: 0.8875513195881615.\n",
      "[I 2025-12-30 21:07:59,761] Trial 11 finished with value: 0.8863583179608262 and parameters: {'iterations': 486, 'learning_rate': 0.2562443345493851, 'depth': 10}. Best is trial 9 with value: 0.8875513195881615.\n",
      "[I 2025-12-30 21:08:13,227] Trial 12 finished with value: 0.8885154168536572 and parameters: {'iterations': 436, 'learning_rate': 0.14673917107280562, 'depth': 9}. Best is trial 12 with value: 0.8885154168536572.\n",
      "[I 2025-12-30 21:08:24,709] Trial 13 finished with value: 0.8871368768114651 and parameters: {'iterations': 375, 'learning_rate': 0.1286813654254574, 'depth': 9}. Best is trial 12 with value: 0.8885154168536572.\n",
      "[I 2025-12-30 21:08:37,598] Trial 14 finished with value: 0.8877845003248046 and parameters: {'iterations': 423, 'learning_rate': 0.13267105039481056, 'depth': 9}. Best is trial 12 with value: 0.8885154168536572.\n",
      "[I 2025-12-30 21:08:45,447] Trial 15 finished with value: 0.8864078947943259 and parameters: {'iterations': 395, 'learning_rate': 0.11976030799372082, 'depth': 8}. Best is trial 12 with value: 0.8885154168536572.\n",
      "[I 2025-12-30 21:08:56,584] Trial 16 finished with value: 0.8785758359395217 and parameters: {'iterations': 566, 'learning_rate': 0.0299292292433652, 'depth': 8}. Best is trial 12 with value: 0.8885154168536572.\n",
      "[I 2025-12-30 21:09:09,806] Trial 17 finished with value: 0.8861578311841517 and parameters: {'iterations': 437, 'learning_rate': 0.08793537314464435, 'depth': 9}. Best is trial 12 with value: 0.8885154168536572.\n",
      "[I 2025-12-30 21:09:16,460] Trial 18 finished with value: 0.886806068323017 and parameters: {'iterations': 328, 'learning_rate': 0.1627467543805639, 'depth': 8}. Best is trial 12 with value: 0.8885154168536572.\n",
      "[I 2025-12-30 21:09:32,854] Trial 19 finished with value: 0.8879178021586983 and parameters: {'iterations': 543, 'learning_rate': 0.08786918993126672, 'depth': 9}. Best is trial 12 with value: 0.8885154168536572.\n",
      "[I 2025-12-30 21:09:43,726] Trial 20 finished with value: 0.8794222439523042 and parameters: {'iterations': 553, 'learning_rate': 0.03235578967595114, 'depth': 8}. Best is trial 12 with value: 0.8885154168536572.\n",
      "[I 2025-12-30 21:09:59,669] Trial 21 finished with value: 0.8872871315493104 and parameters: {'iterations': 522, 'learning_rate': 0.09820987945474322, 'depth': 9}. Best is trial 12 with value: 0.8885154168536572.\n",
      "[I 2025-12-30 21:10:17,648] Trial 22 finished with value: 0.8877175735768391 and parameters: {'iterations': 593, 'learning_rate': 0.1693585644439109, 'depth': 9}. Best is trial 12 with value: 0.8885154168536572.\n",
      "[I 2025-12-30 21:10:39,643] Trial 23 finished with value: 0.8844003441094258 and parameters: {'iterations': 370, 'learning_rate': 0.05915287953657093, 'depth': 10}. Best is trial 12 with value: 0.8885154168536572.\n",
      "[I 2025-12-30 21:10:48,263] Trial 24 finished with value: 0.8852974179054547 and parameters: {'iterations': 433, 'learning_rate': 0.10692321299763922, 'depth': 8}. Best is trial 12 with value: 0.8885154168536572.\n",
      "[I 2025-12-30 21:11:02,802] Trial 25 finished with value: 0.8878350590456869 and parameters: {'iterations': 478, 'learning_rate': 0.1856452408836201, 'depth': 9}. Best is trial 12 with value: 0.8885154168536572.\n",
      "[I 2025-12-30 21:11:10,427] Trial 26 finished with value: 0.8866731400295881 and parameters: {'iterations': 511, 'learning_rate': 0.18250116134548194, 'depth': 7}. Best is trial 12 with value: 0.8885154168536572.\n",
      "[I 2025-12-30 21:11:38,026] Trial 27 finished with value: 0.8882823809420688 and parameters: {'iterations': 462, 'learning_rate': 0.1812224464842411, 'depth': 10}. Best is trial 12 with value: 0.8885154168536572.\n",
      "[I 2025-12-30 21:12:12,685] Trial 28 finished with value: 0.8871879103421739 and parameters: {'iterations': 585, 'learning_rate': 0.06321047523866453, 'depth': 10}. Best is trial 12 with value: 0.8885154168536572.\n",
      "[I 2025-12-30 21:12:39,618] Trial 29 finished with value: 0.8821105710845151 and parameters: {'iterations': 453, 'learning_rate': 0.034906369475251824, 'depth': 10}. Best is trial 12 with value: 0.8885154168536572.\n",
      "[I 2025-12-30 21:13:00,217] A new study created in memory with name: no-name-5567dc26-980c-4761-b3e9-d49beb7a707b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULTS FOR: CatBoost\n",
      "----------------------------------------\n",
      "ACCURACY     : 0.8823\n",
      "PRECISION    : 0.8588\n",
      "RECALL       : 0.8218\n",
      "F1 SCORE     : 0.8399\n",
      "K-FOLD MEAN  : 0.9005\n",
      "K-FOLD STD   : 0.0007\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "ðŸ”¥ NOW TUNING: HistGradientBoosting\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-30 21:13:02,799] Trial 0 finished with value: 0.8691168351585673 and parameters: {'learning_rate': 0.08147071375401341, 'max_depth': 10}. Best is trial 0 with value: 0.8691168351585673.\n",
      "[I 2025-12-30 21:13:05,043] Trial 1 finished with value: 0.8467662020306318 and parameters: {'learning_rate': 0.06145318560222049, 'max_depth': 6}. Best is trial 0 with value: 0.8691168351585673.\n",
      "[I 2025-12-30 21:13:07,122] Trial 2 finished with value: 0.8020249414641346 and parameters: {'learning_rate': 0.03208255889494792, 'max_depth': 5}. Best is trial 0 with value: 0.8691168351585673.\n",
      "[I 2025-12-30 21:13:09,789] Trial 3 finished with value: 0.8041327066132048 and parameters: {'learning_rate': 0.01787166274054351, 'max_depth': 9}. Best is trial 0 with value: 0.8691168351585673.\n",
      "[I 2025-12-30 21:13:12,363] Trial 4 finished with value: 0.7937624272362614 and parameters: {'learning_rate': 0.01329048761780171, 'max_depth': 8}. Best is trial 0 with value: 0.8691168351585673.\n",
      "[I 2025-12-30 21:13:14,620] Trial 5 finished with value: 0.8801362262013536 and parameters: {'learning_rate': 0.19609221646805147, 'max_depth': 10}. Best is trial 5 with value: 0.8801362262013536.\n",
      "[I 2025-12-30 21:13:17,226] Trial 6 finished with value: 0.822383364731642 and parameters: {'learning_rate': 0.026248316686909662, 'max_depth': 9}. Best is trial 5 with value: 0.8801362262013536.\n",
      "[I 2025-12-30 21:13:19,933] Trial 7 finished with value: 0.7997975705257269 and parameters: {'learning_rate': 0.012426530258963472, 'max_depth': 10}. Best is trial 5 with value: 0.8801362262013536.\n",
      "[I 2025-12-30 21:13:22,211] Trial 8 finished with value: 0.8805341548807699 and parameters: {'learning_rate': 0.18156576374281408, 'max_depth': 8}. Best is trial 8 with value: 0.8805341548807699.\n",
      "[I 2025-12-30 21:13:24,624] Trial 9 finished with value: 0.8709595086951097 and parameters: {'learning_rate': 0.08863159174528622, 'max_depth': 8}. Best is trial 8 with value: 0.8805341548807699.\n",
      "[I 2025-12-30 21:13:26,277] Trial 10 finished with value: 0.8582322235449622 and parameters: {'learning_rate': 0.18879063562768889, 'max_depth': 3}. Best is trial 8 with value: 0.8805341548807699.\n",
      "[I 2025-12-30 21:13:28,510] Trial 11 finished with value: 0.8813123958470358 and parameters: {'learning_rate': 0.19650488743549058, 'max_depth': 7}. Best is trial 11 with value: 0.8813123958470358.\n",
      "[I 2025-12-30 21:13:30,686] Trial 12 finished with value: 0.8733646857945238 and parameters: {'learning_rate': 0.12933210408037746, 'max_depth': 6}. Best is trial 11 with value: 0.8813123958470358.\n",
      "[I 2025-12-30 21:13:32,501] Trial 13 finished with value: 0.8590600431042638 and parameters: {'learning_rate': 0.13483627834584833, 'max_depth': 4}. Best is trial 11 with value: 0.8813123958470358.\n",
      "[I 2025-12-30 21:13:34,803] Trial 14 finished with value: 0.8751242335818693 and parameters: {'learning_rate': 0.11856685785527753, 'max_depth': 7}. Best is trial 11 with value: 0.8813123958470358.\n",
      "[I 2025-12-30 21:13:36,304] Trial 15 finished with value: 0.779867226091873 and parameters: {'learning_rate': 0.04658219784104742, 'max_depth': 2}. Best is trial 11 with value: 0.8813123958470358.\n",
      "[I 2025-12-30 21:13:38,526] Trial 16 finished with value: 0.8809149686685268 and parameters: {'learning_rate': 0.1978892909075731, 'max_depth': 7}. Best is trial 11 with value: 0.8813123958470358.\n",
      "[I 2025-12-30 21:13:40,596] Trial 17 finished with value: 0.8514106102986867 and parameters: {'learning_rate': 0.08321498720493091, 'max_depth': 5}. Best is trial 11 with value: 0.8813123958470358.\n",
      "[I 2025-12-30 21:13:42,958] Trial 18 finished with value: 0.8524381493864022 and parameters: {'learning_rate': 0.05650947840419615, 'max_depth': 7}. Best is trial 11 with value: 0.8813123958470358.\n",
      "[I 2025-12-30 21:13:45,279] Trial 19 finished with value: 0.8779954074117846 and parameters: {'learning_rate': 0.13310886586267912, 'max_depth': 7}. Best is trial 11 with value: 0.8813123958470358.\n",
      "[I 2025-12-30 21:13:47,337] Trial 20 finished with value: 0.8585913307538542 and parameters: {'learning_rate': 0.10050593886175135, 'max_depth': 5}. Best is trial 11 with value: 0.8813123958470358.\n",
      "[I 2025-12-30 21:13:49,595] Trial 21 finished with value: 0.8822932324061495 and parameters: {'learning_rate': 0.19666705420595068, 'max_depth': 8}. Best is trial 21 with value: 0.8822932324061495.\n",
      "[I 2025-12-30 21:13:51,863] Trial 22 finished with value: 0.8794224049112239 and parameters: {'learning_rate': 0.1580771857837782, 'max_depth': 7}. Best is trial 21 with value: 0.8822932324061495.\n",
      "[I 2025-12-30 21:13:54,217] Trial 23 finished with value: 0.878309876107413 and parameters: {'learning_rate': 0.15654816776037364, 'max_depth': 9}. Best is trial 21 with value: 0.8822932324061495.\n",
      "[I 2025-12-30 21:13:56,465] Trial 24 finished with value: 0.883156732354936 and parameters: {'learning_rate': 0.19814706187780934, 'max_depth': 8}. Best is trial 24 with value: 0.883156732354936.\n",
      "[I 2025-12-30 21:13:58,847] Trial 25 finished with value: 0.8733167210405037 and parameters: {'learning_rate': 0.1059604350893245, 'max_depth': 8}. Best is trial 24 with value: 0.883156732354936.\n",
      "[I 2025-12-30 21:14:01,325] Trial 26 finished with value: 0.8650356443194828 and parameters: {'learning_rate': 0.06916498273770559, 'max_depth': 9}. Best is trial 24 with value: 0.883156732354936.\n",
      "[I 2025-12-30 21:14:03,528] Trial 27 finished with value: 0.8750078985118618 and parameters: {'learning_rate': 0.15301069382448163, 'max_depth': 6}. Best is trial 24 with value: 0.883156732354936.\n",
      "[I 2025-12-30 21:14:05,999] Trial 28 finished with value: 0.832043599153847 and parameters: {'learning_rate': 0.033127896019999756, 'max_depth': 8}. Best is trial 24 with value: 0.883156732354936.\n",
      "[I 2025-12-30 21:14:08,508] Trial 29 finished with value: 0.8740113672407485 and parameters: {'learning_rate': 0.10451107629109826, 'max_depth': 10}. Best is trial 24 with value: 0.883156732354936.\n",
      "[I 2025-12-30 21:14:11,433] A new study created in memory with name: no-name-8fb64c1a-f61c-4280-ba14-0a5dff615086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULTS FOR: HistGradientBoosting\n",
      "----------------------------------------\n",
      "ACCURACY     : 0.8730\n",
      "PRECISION    : 0.8360\n",
      "RECALL       : 0.8235\n",
      "F1 SCORE     : 0.8297\n",
      "K-FOLD MEAN  : 0.8918\n",
      "K-FOLD STD   : 0.0015\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "ðŸ”¥ NOW TUNING: MLPC\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (50,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "[I 2025-12-30 21:15:09,579] Trial 0 finished with value: 0.839622141144379 and parameters: {'hidden_layer_sizes': (50,), 'alpha': 0.007761740290734759, 'lr': 0.0002634015038023835}. Best is trial 0 with value: 0.839622141144379.\n",
      "[I 2025-12-30 21:16:43,168] Trial 1 finished with value: 0.8159240431662992 and parameters: {'hidden_layer_sizes': (100,), 'alpha': 0.014929868311637786, 'lr': 0.0012980949160219329}. Best is trial 0 with value: 0.839622141144379.\n",
      "[I 2025-12-30 21:17:51,051] Trial 2 finished with value: 0.8275680196211015 and parameters: {'hidden_layer_sizes': (100, 50), 'alpha': 0.04451689136196366, 'lr': 0.0017375402014233106}. Best is trial 0 with value: 0.839622141144379.\n",
      "[I 2025-12-30 21:20:57,692] Trial 3 finished with value: 0.8467044188495786 and parameters: {'hidden_layer_sizes': (100, 50), 'alpha': 0.005323638639902622, 'lr': 0.003960290871626454}. Best is trial 3 with value: 0.8467044188495786.\n",
      "[I 2025-12-30 21:22:27,246] Trial 4 finished with value: 0.8507941750477981 and parameters: {'hidden_layer_sizes': (100,), 'alpha': 0.001055656485879099, 'lr': 0.0008953702244027214}. Best is trial 4 with value: 0.8507941750477981.\n",
      "[I 2025-12-30 21:23:03,430] Trial 5 finished with value: 0.8457210626299202 and parameters: {'hidden_layer_sizes': (50,), 'alpha': 0.008063006155852112, 'lr': 0.0016536053827271688}. Best is trial 4 with value: 0.8507941750477981.\n",
      "[I 2025-12-30 21:31:42,369] Trial 6 finished with value: 0.8341744315352594 and parameters: {'hidden_layer_sizes': (100, 50), 'alpha': 0.04072433404935312, 'lr': 0.0015495094510996764}. Best is trial 4 with value: 0.8507941750477981.\n",
      "[I 2025-12-30 21:32:23,907] Trial 7 finished with value: 0.8181091599584867 and parameters: {'hidden_layer_sizes': (50,), 'alpha': 0.00042464490292726904, 'lr': 0.0011887195800597144}. Best is trial 4 with value: 0.8507941750477981.\n",
      "[I 2025-12-30 21:32:42,212] Trial 8 finished with value: 0.7812531555719975 and parameters: {'hidden_layer_sizes': (100,), 'alpha': 0.7927852282561486, 'lr': 0.00012064678773327204}. Best is trial 4 with value: 0.8507941750477981.\n",
      "[I 2025-12-30 21:36:42,477] Trial 9 finished with value: 0.8540672603326778 and parameters: {'hidden_layer_sizes': (100, 50), 'alpha': 0.0002080379953777523, 'lr': 0.00029389734813592075}. Best is trial 9 with value: 0.8540672603326778.\n",
      "[I 2025-12-30 21:40:26,148] Trial 10 finished with value: 0.8628468735253522 and parameters: {'hidden_layer_sizes': (100, 50), 'alpha': 0.00015219201465438878, 'lr': 0.0003639589211654078}. Best is trial 10 with value: 0.8628468735253522.\n",
      "[I 2025-12-30 21:44:20,780] Trial 11 finished with value: 0.8492851938369775 and parameters: {'hidden_layer_sizes': (100, 50), 'alpha': 0.00013275870976890168, 'lr': 0.00032933353559430243}. Best is trial 10 with value: 0.8628468735253522.\n",
      "[I 2025-12-30 21:48:03,008] Trial 12 finished with value: 0.8353671315728294 and parameters: {'hidden_layer_sizes': (100, 50), 'alpha': 0.00013112543002204713, 'lr': 0.0003808185009816647}. Best is trial 10 with value: 0.8628468735253522.\n",
      "[I 2025-12-30 21:51:41,974] Trial 13 finished with value: 0.8503124241360408 and parameters: {'hidden_layer_sizes': (100, 50), 'alpha': 0.001180896548049455, 'lr': 0.0001125093582200361}. Best is trial 10 with value: 0.8628468735253522.\n",
      "[I 2025-12-30 21:55:44,773] Trial 14 finished with value: 0.8591621934325606 and parameters: {'hidden_layer_sizes': (100, 50), 'alpha': 0.0004906448523623187, 'lr': 0.0006727807324598166}. Best is trial 10 with value: 0.8628468735253522.\n",
      "[I 2025-12-30 21:59:50,507] Trial 15 finished with value: 0.8489286596968691 and parameters: {'hidden_layer_sizes': (100, 50), 'alpha': 0.0012850786113174177, 'lr': 0.0006247567941554917}. Best is trial 10 with value: 0.8628468735253522.\n",
      "[I 2025-12-30 22:03:37,797] Trial 16 finished with value: 0.8656473496322064 and parameters: {'hidden_layer_sizes': (100, 50), 'alpha': 0.0004223642103677021, 'lr': 0.0038490083012672678}. Best is trial 16 with value: 0.8656473496322064.\n",
      "[I 2025-12-30 22:07:07,881] Trial 17 finished with value: 0.8560418160378482 and parameters: {'hidden_layer_sizes': (100, 50), 'alpha': 0.0020871648662734044, 'lr': 0.007188274552255868}. Best is trial 16 with value: 0.8656473496322064.\n",
      "[I 2025-12-30 22:09:10,969] Trial 18 finished with value: 0.8387890193574433 and parameters: {'hidden_layer_sizes': (100,), 'alpha': 0.0004162669006764503, 'lr': 0.003073102182087001}. Best is trial 16 with value: 0.8656473496322064.\n",
      "[I 2025-12-30 22:09:17,452] Trial 19 finished with value: 0.7708972374890776 and parameters: {'hidden_layer_sizes': (50,), 'alpha': 0.3109751108498025, 'lr': 0.007008557261925962}. Best is trial 16 with value: 0.8656473496322064.\n",
      "[I 2025-12-30 22:12:52,246] Trial 20 finished with value: 0.8580482203847678 and parameters: {'hidden_layer_sizes': (100, 50), 'alpha': 0.002792528320320205, 'lr': 0.003405858383506951}. Best is trial 16 with value: 0.8656473496322064.\n",
      "[I 2025-12-30 22:17:21,118] Trial 21 finished with value: 0.8370156217164801 and parameters: {'hidden_layer_sizes': (100, 50), 'alpha': 0.00035074047537965827, 'lr': 0.0005928114491114513}. Best is trial 16 with value: 0.8656473496322064.\n",
      "[I 2025-12-30 22:21:48,879] Trial 22 finished with value: 0.8470118435194308 and parameters: {'hidden_layer_sizes': (100, 50), 'alpha': 0.0006073619233341537, 'lr': 0.000535203286478604}. Best is trial 16 with value: 0.8656473496322064.\n",
      "[I 2025-12-30 22:25:06,882] Trial 23 finished with value: 0.8553526430369526 and parameters: {'hidden_layer_sizes': (100, 50), 'alpha': 0.00012252407447751643, 'lr': 0.00019597420679188919}. Best is trial 16 with value: 0.8656473496322064.\n",
      "[I 2025-12-30 22:28:28,979] Trial 24 finished with value: 0.8582636090975546 and parameters: {'hidden_layer_sizes': (100, 50), 'alpha': 0.0002548583841164273, 'lr': 0.0008900623405974262}. Best is trial 16 with value: 0.8656473496322064.\n",
      "[I 2025-12-30 22:31:48,150] Trial 25 finished with value: 0.854367830608402 and parameters: {'hidden_layer_sizes': (100, 50), 'alpha': 0.0005681913587371137, 'lr': 0.002204858273740168}. Best is trial 16 with value: 0.8656473496322064.\n",
      "[I 2025-12-30 22:35:47,938] Trial 26 finished with value: 0.8119497595545818 and parameters: {'hidden_layer_sizes': (100, 50), 'alpha': 0.0035930038226296484, 'lr': 0.00020782427076456656}. Best is trial 16 with value: 0.8656473496322064.\n",
      "[I 2025-12-30 22:39:57,607] Trial 27 finished with value: 0.8607812189483478 and parameters: {'hidden_layer_sizes': (100, 50), 'alpha': 0.00010030450913746051, 'lr': 0.005131863528643382}. Best is trial 16 with value: 0.8656473496322064.\n",
      "[I 2025-12-30 22:41:25,563] Trial 28 finished with value: 0.831566824292358 and parameters: {'hidden_layer_sizes': (100,), 'alpha': 0.00010137864559700637, 'lr': 0.005045588722544734}. Best is trial 16 with value: 0.8656473496322064.\n",
      "[I 2025-12-30 22:42:07,471] Trial 29 finished with value: 0.8448374704569949 and parameters: {'hidden_layer_sizes': (50,), 'alpha': 0.0002306734176417682, 'lr': 0.005008583103541649}. Best is trial 16 with value: 0.8656473496322064.\n",
      "[I 2025-12-30 22:48:57,214] A new study created in memory with name: no-name-418888e7-d019-4c33-961c-079acee9d0e0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULTS FOR: MLPC\n",
      "----------------------------------------\n",
      "ACCURACY     : 0.8373\n",
      "PRECISION    : 0.7836\n",
      "RECALL       : 0.7831\n",
      "F1 SCORE     : 0.7833\n",
      "K-FOLD MEAN  : 0.8451\n",
      "K-FOLD STD   : 0.0017\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "ðŸ”¥ NOW TUNING: Hard Voting\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-30 22:49:11,491] Trial 0 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:49:25,643] Trial 1 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:49:39,897] Trial 2 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:49:54,179] Trial 3 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:50:08,182] Trial 4 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:50:22,172] Trial 5 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:50:36,306] Trial 6 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:50:48,458] Trial 7 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:51:00,527] Trial 8 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:51:12,491] Trial 9 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:51:24,539] Trial 10 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:51:36,617] Trial 11 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:51:48,910] Trial 12 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:52:00,925] Trial 13 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:52:12,977] Trial 14 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:52:24,992] Trial 15 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:52:36,974] Trial 16 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:52:48,976] Trial 17 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:53:01,106] Trial 18 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:53:13,098] Trial 19 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:53:25,053] Trial 20 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:53:37,050] Trial 21 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:53:49,153] Trial 22 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:54:01,157] Trial 23 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:54:13,212] Trial 24 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:54:25,283] Trial 25 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:54:37,308] Trial 26 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:54:49,385] Trial 27 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:55:01,441] Trial 28 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n",
      "[I 2025-12-30 22:55:13,542] Trial 29 finished with value: 0.8826755202701216 and parameters: {}. Best is trial 0 with value: 0.8826755202701216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 60259, number of negative: 60259\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010439 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7341\n",
      "[LightGBM] [Info] Number of data points in the train set: 120518, number of used features: 49\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-30 22:55:41,345] A new study created in memory with name: no-name-3d481cbd-fce4-4748-9a34-0cf86ba94cea\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULTS FOR: Hard Voting\n",
      "----------------------------------------\n",
      "ACCURACY     : 0.8777\n",
      "PRECISION    : 0.8505\n",
      "RECALL       : 0.8184\n",
      "F1 SCORE     : 0.8341\n",
      "K-FOLD MEAN  : 0.8963\n",
      "K-FOLD STD   : 0.0017\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "ðŸ”¥ NOW TUNING: RidgeClassifier\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-30 22:55:41,726] Trial 0 finished with value: 0.7674231368041453 and parameters: {'alpha': 3.2619979433384394}. Best is trial 0 with value: 0.7674231368041453.\n",
      "[I 2025-12-30 22:55:42,101] Trial 1 finished with value: 0.767406140560854 and parameters: {'alpha': 0.029436669701662536}. Best is trial 0 with value: 0.7674231368041453.\n",
      "[I 2025-12-30 22:55:42,484] Trial 2 finished with value: 0.7674726985325687 and parameters: {'alpha': 0.06519580026321113}. Best is trial 2 with value: 0.7674726985325687.\n",
      "[I 2025-12-30 22:55:42,863] Trial 3 finished with value: 0.7671571023937557 and parameters: {'alpha': 0.010606703893404307}. Best is trial 2 with value: 0.7674726985325687.\n",
      "[I 2025-12-30 22:55:43,238] Trial 4 finished with value: 0.767555579432491 and parameters: {'alpha': 0.052334307875620954}. Best is trial 4 with value: 0.767555579432491.\n",
      "[I 2025-12-30 22:55:43,624] Trial 5 finished with value: 0.7676556442257048 and parameters: {'alpha': 0.9236228573251605}. Best is trial 5 with value: 0.7676556442257048.\n",
      "[I 2025-12-30 22:55:44,007] Trial 6 finished with value: 0.76730664202036 and parameters: {'alpha': 0.0200652903402764}. Best is trial 5 with value: 0.7676556442257048.\n",
      "[I 2025-12-30 22:55:44,388] Trial 7 finished with value: 0.7673401123243101 and parameters: {'alpha': 5.254641780248095}. Best is trial 5 with value: 0.7676556442257048.\n",
      "[I 2025-12-30 22:55:44,766] Trial 8 finished with value: 0.7676056633435467 and parameters: {'alpha': 0.5803861272016627}. Best is trial 5 with value: 0.7676556442257048.\n",
      "[I 2025-12-30 22:55:45,150] Trial 9 finished with value: 0.7674892872081296 and parameters: {'alpha': 0.20912496070199457}. Best is trial 5 with value: 0.7676556442257048.\n",
      "[I 2025-12-30 22:55:45,540] Trial 10 finished with value: 0.7677053228429603 and parameters: {'alpha': 0.9872042145889807}. Best is trial 10 with value: 0.7677053228429603.\n",
      "[I 2025-12-30 22:55:45,922] Trial 11 finished with value: 0.7676886837899158 and parameters: {'alpha': 1.0233057902023643}. Best is trial 10 with value: 0.7677053228429603.\n",
      "[I 2025-12-30 22:55:46,301] Trial 12 finished with value: 0.7674894539600513 and parameters: {'alpha': 1.6047945945065223}. Best is trial 10 with value: 0.7677053228429603.\n",
      "[I 2025-12-30 22:55:46,690] Trial 13 finished with value: 0.7674561138904741 and parameters: {'alpha': 0.2250692230375835}. Best is trial 10 with value: 0.7677053228429603.\n",
      "[I 2025-12-30 22:55:47,069] Trial 14 finished with value: 0.767506060014619 and parameters: {'alpha': 1.588875735428068}. Best is trial 10 with value: 0.7677053228429603.\n",
      "[I 2025-12-30 22:55:47,447] Trial 15 finished with value: 0.7673896665001951 and parameters: {'alpha': 0.32284147232857896}. Best is trial 10 with value: 0.7677053228429603.\n",
      "[I 2025-12-30 22:55:47,822] Trial 16 finished with value: 0.7671907064511503 and parameters: {'alpha': 5.781419231141811}. Best is trial 10 with value: 0.7677053228429603.\n",
      "[I 2025-12-30 22:55:48,201] Trial 17 finished with value: 0.767489812909834 and parameters: {'alpha': 2.3664306148488614}. Best is trial 10 with value: 0.7677053228429603.\n",
      "[I 2025-12-30 22:55:48,577] Trial 18 finished with value: 0.7676056633435467 and parameters: {'alpha': 0.6195513187098958}. Best is trial 10 with value: 0.7677053228429603.\n",
      "[I 2025-12-30 22:55:48,951] Trial 19 finished with value: 0.767539128543841 and parameters: {'alpha': 0.11282224012554244}. Best is trial 10 with value: 0.7677053228429603.\n",
      "[I 2025-12-30 22:55:49,333] Trial 20 finished with value: 0.7676886779969135 and parameters: {'alpha': 1.0342007113866867}. Best is trial 10 with value: 0.7677053228429603.\n",
      "[I 2025-12-30 22:55:49,715] Trial 21 finished with value: 0.7676058086829958 and parameters: {'alpha': 0.8777466751380608}. Best is trial 10 with value: 0.7677053228429603.\n",
      "[I 2025-12-30 22:55:50,095] Trial 22 finished with value: 0.7673399201264491 and parameters: {'alpha': 0.425878887353411}. Best is trial 10 with value: 0.7677053228429603.\n",
      "[I 2025-12-30 22:55:50,476] Trial 23 finished with value: 0.7671741663935371 and parameters: {'alpha': 9.085130805651602}. Best is trial 10 with value: 0.7677053228429603.\n",
      "[I 2025-12-30 22:55:50,862] Trial 24 finished with value: 0.7677551525926672 and parameters: {'alpha': 1.1494452110502198}. Best is trial 24 with value: 0.7677551525926672.\n",
      "[I 2025-12-30 22:55:51,240] Trial 25 finished with value: 0.7674399501616499 and parameters: {'alpha': 2.275364147574927}. Best is trial 24 with value: 0.7677551525926672.\n",
      "[I 2025-12-30 22:55:51,619] Trial 26 finished with value: 0.767489498544533 and parameters: {'alpha': 1.5100344974790065}. Best is trial 24 with value: 0.7677551525926672.\n",
      "[I 2025-12-30 22:55:51,998] Trial 27 finished with value: 0.7674896907423937 and parameters: {'alpha': 3.7640295475183345}. Best is trial 24 with value: 0.7677551525926672.\n",
      "[I 2025-12-30 22:55:52,377] Trial 28 finished with value: 0.7674062007648063 and parameters: {'alpha': 0.16486564317252556}. Best is trial 24 with value: 0.7677551525926672.\n",
      "[I 2025-12-30 22:55:52,767] Trial 29 finished with value: 0.7673399201264491 and parameters: {'alpha': 0.41053962411718964}. Best is trial 24 with value: 0.7677551525926672.\n",
      "[I 2025-12-30 22:55:53,256] A new study created in memory with name: no-name-78c1a2ce-8f8f-473b-a6de-e2732e042fce\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULTS FOR: RidgeClassifier\n",
      "----------------------------------------\n",
      "ACCURACY     : 0.7971\n",
      "PRECISION    : 0.7143\n",
      "RECALL       : 0.7666\n",
      "F1 SCORE     : 0.7395\n",
      "K-FOLD MEAN  : 0.7894\n",
      "K-FOLD STD   : 0.0009\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "ðŸ”¥ NOW TUNING: Soft Voting\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-30 22:56:05,311] Trial 0 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 22:56:17,322] Trial 1 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 22:56:29,322] Trial 2 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 22:56:41,284] Trial 3 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 22:56:53,203] Trial 4 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 22:57:05,092] Trial 5 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 22:57:17,056] Trial 6 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 22:57:29,182] Trial 7 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 22:57:41,086] Trial 8 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 22:57:53,097] Trial 9 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 22:58:05,048] Trial 10 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 22:58:17,043] Trial 11 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 22:58:29,006] Trial 12 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 22:58:40,971] Trial 13 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 22:58:52,918] Trial 14 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 22:59:04,882] Trial 15 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 22:59:16,903] Trial 16 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 22:59:28,793] Trial 17 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 22:59:40,717] Trial 18 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 22:59:52,593] Trial 19 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 23:00:04,574] Trial 20 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 23:00:16,685] Trial 21 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 23:00:28,606] Trial 22 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 23:00:40,633] Trial 23 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 23:00:52,591] Trial 24 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 23:01:04,558] Trial 25 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 23:01:16,523] Trial 26 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 23:01:28,438] Trial 27 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 23:01:40,662] Trial 28 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n",
      "[I 2025-12-30 23:01:52,754] Trial 29 finished with value: 0.8867240345282442 and parameters: {}. Best is trial 0 with value: 0.8867240345282442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 60259, number of negative: 60259\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005562 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7341\n",
      "[LightGBM] [Info] Number of data points in the train set: 120518, number of used features: 49\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-30 23:02:20,487] A new study created in memory with name: no-name-a80884fc-1d1f-4d57-8fb8-ba1527b5bf28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULTS FOR: Soft Voting\n",
      "----------------------------------------\n",
      "ACCURACY     : 0.8827\n",
      "PRECISION    : 0.8597\n",
      "RECALL       : 0.8220\n",
      "F1 SCORE     : 0.8404\n",
      "K-FOLD MEAN  : 0.9004\n",
      "K-FOLD STD   : 0.0016\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "ðŸ”¥ NOW TUNING: RandomForest\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-30 23:02:22,656] Trial 0 finished with value: 0.8464149134019351 and parameters: {'n_estimators': 104, 'max_depth': 14, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 0 with value: 0.8464149134019351.\n",
      "[I 2025-12-30 23:02:25,175] Trial 1 finished with value: 0.8718884686518314 and parameters: {'n_estimators': 121, 'max_depth': 27, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 1 with value: 0.8718884686518314.\n",
      "[I 2025-12-30 23:02:32,285] Trial 2 finished with value: 0.8805506433157575 and parameters: {'n_estimators': 279, 'max_depth': 29, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8805506433157575.\n",
      "[I 2025-12-30 23:02:38,091] Trial 3 finished with value: 0.8379700058737107 and parameters: {'n_estimators': 325, 'max_depth': 13, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8805506433157575.\n",
      "[I 2025-12-30 23:02:51,202] Trial 4 finished with value: 0.8653842916085112 and parameters: {'n_estimators': 595, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 4, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8805506433157575.\n",
      "[I 2025-12-30 23:02:55,913] Trial 5 finished with value: 0.864555512088694 and parameters: {'n_estimators': 251, 'max_depth': 25, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 2 with value: 0.8805506433157575.\n",
      "[I 2025-12-30 23:02:58,543] Trial 6 finished with value: 0.7633071676060693 and parameters: {'n_estimators': 356, 'max_depth': 5, 'min_samples_split': 7, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 2 with value: 0.8805506433157575.\n",
      "[I 2025-12-30 23:03:03,100] Trial 7 finished with value: 0.862015023550998 and parameters: {'n_estimators': 258, 'max_depth': 19, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 2 with value: 0.8805506433157575.\n",
      "[I 2025-12-30 23:03:06,694] Trial 8 finished with value: 0.7743092443048382 and parameters: {'n_estimators': 346, 'max_depth': 6, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 2 with value: 0.8805506433157575.\n",
      "[I 2025-12-30 23:03:08,795] Trial 9 finished with value: 0.779108598291416 and parameters: {'n_estimators': 216, 'max_depth': 7, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 'log2'}. Best is trial 2 with value: 0.8805506433157575.\n",
      "[I 2025-12-30 23:03:21,736] Trial 10 finished with value: 0.8844174370742185 and parameters: {'n_estimators': 506, 'max_depth': 30, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8844174370742185.\n",
      "[I 2025-12-30 23:03:34,523] Trial 11 finished with value: 0.88433437783637 and parameters: {'n_estimators': 501, 'max_depth': 30, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.8844174370742185.\n",
      "[I 2025-12-30 23:03:47,879] Trial 12 finished with value: 0.8859607829892054 and parameters: {'n_estimators': 520, 'max_depth': 30, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8859607829892054.\n",
      "[I 2025-12-30 23:03:59,048] Trial 13 finished with value: 0.8725367765518648 and parameters: {'n_estimators': 478, 'max_depth': 23, 'min_samples_split': 2, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8859607829892054.\n",
      "[I 2025-12-30 23:04:10,069] Trial 14 finished with value: 0.880501687876675 and parameters: {'n_estimators': 453, 'max_depth': 24, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8859607829892054.\n",
      "[I 2025-12-30 23:04:23,912] Trial 15 finished with value: 0.8771998677209716 and parameters: {'n_estimators': 565, 'max_depth': 30, 'min_samples_split': 4, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8859607829892054.\n",
      "[I 2025-12-30 23:04:34,251] Trial 16 finished with value: 0.8780126779837559 and parameters: {'n_estimators': 437, 'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8859607829892054.\n",
      "[I 2025-12-30 23:04:47,488] Trial 17 finished with value: 0.8814465296810026 and parameters: {'n_estimators': 533, 'max_depth': 26, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8859607829892054.\n",
      "[I 2025-12-30 23:04:55,675] Trial 18 finished with value: 0.854116059360701 and parameters: {'n_estimators': 404, 'max_depth': 16, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8859607829892054.\n",
      "[I 2025-12-30 23:05:08,648] Trial 19 finished with value: 0.8789748681593665 and parameters: {'n_estimators': 527, 'max_depth': 27, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8859607829892054.\n",
      "[I 2025-12-30 23:05:14,811] Trial 20 finished with value: 0.814057741156604 and parameters: {'n_estimators': 413, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8859607829892054.\n",
      "[I 2025-12-30 23:05:27,623] Trial 21 finished with value: 0.88433437783637 and parameters: {'n_estimators': 501, 'max_depth': 30, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8859607829892054.\n",
      "[I 2025-12-30 23:05:42,566] Trial 22 finished with value: 0.8822936805248952 and parameters: {'n_estimators': 595, 'max_depth': 28, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8859607829892054.\n",
      "[I 2025-12-30 23:05:56,068] Trial 23 finished with value: 0.8815129075552551 and parameters: {'n_estimators': 530, 'max_depth': 30, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8859607829892054.\n",
      "[I 2025-12-30 23:06:07,808] Trial 24 finished with value: 0.8786762508848383 and parameters: {'n_estimators': 478, 'max_depth': 23, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8859607829892054.\n",
      "[I 2025-12-30 23:06:18,078] Trial 25 finished with value: 0.8855461381881801 and parameters: {'n_estimators': 397, 'max_depth': 27, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8859607829892054.\n",
      "[I 2025-12-30 23:06:25,858] Trial 26 finished with value: 0.8753902921930562 and parameters: {'n_estimators': 387, 'max_depth': 26, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2'}. Best is trial 12 with value: 0.8859607829892054.\n",
      "[I 2025-12-30 23:06:36,654] Trial 27 finished with value: 0.880136296231774 and parameters: {'n_estimators': 446, 'max_depth': 22, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8859607829892054.\n",
      "[I 2025-12-30 23:06:50,415] Trial 28 finished with value: 0.8812145549992915 and parameters: {'n_estimators': 546, 'max_depth': 28, 'min_samples_split': 3, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8859607829892054.\n",
      "[I 2025-12-30 23:06:58,799] Trial 29 finished with value: 0.8653338162635899 and parameters: {'n_estimators': 378, 'max_depth': 18, 'min_samples_split': 6, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 12 with value: 0.8859607829892054.\n",
      "[I 2025-12-30 23:07:19,379] A new study created in memory with name: no-name-b03bd20a-fe73-4ca6-ac68-3365b248d821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULTS FOR: RandomForest\n",
      "----------------------------------------\n",
      "ACCURACY     : 0.8844\n",
      "PRECISION    : 0.8676\n",
      "RECALL       : 0.8169\n",
      "F1 SCORE     : 0.8415\n",
      "K-FOLD MEAN  : 0.9017\n",
      "K-FOLD STD   : 0.0014\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "ðŸ”¥ NOW TUNING: DecisionTree\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-30 23:07:19,834] Trial 0 finished with value: 0.326607655406198 and parameters: {'max_depth': 2, 'min_samples_split': 18, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.326607655406198.\n",
      "[I 2025-12-30 23:07:20,983] Trial 1 finished with value: 0.8452389407183271 and parameters: {'max_depth': 29, 'min_samples_split': 13, 'min_samples_leaf': 4}. Best is trial 1 with value: 0.8452389407183271.\n",
      "[I 2025-12-30 23:07:21,823] Trial 2 finished with value: 0.7898620788507887 and parameters: {'max_depth': 10, 'min_samples_split': 15, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.8452389407183271.\n",
      "[I 2025-12-30 23:07:22,502] Trial 3 finished with value: 0.7934194907400434 and parameters: {'max_depth': 7, 'min_samples_split': 14, 'min_samples_leaf': 6}. Best is trial 1 with value: 0.8452389407183271.\n",
      "[I 2025-12-30 23:07:23,601] Trial 4 finished with value: 0.8476573571622246 and parameters: {'max_depth': 22, 'min_samples_split': 20, 'min_samples_leaf': 2}. Best is trial 4 with value: 0.8476573571622246.\n",
      "[I 2025-12-30 23:07:24,727] Trial 5 finished with value: 0.8530964206042709 and parameters: {'max_depth': 24, 'min_samples_split': 14, 'min_samples_leaf': 3}. Best is trial 5 with value: 0.8530964206042709.\n",
      "[I 2025-12-30 23:07:25,838] Trial 6 finished with value: 0.8505069976889467 and parameters: {'max_depth': 26, 'min_samples_split': 7, 'min_samples_leaf': 9}. Best is trial 5 with value: 0.8530964206042709.\n",
      "[I 2025-12-30 23:07:26,997] Trial 7 finished with value: 0.8418496711475049 and parameters: {'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 5 with value: 0.8530964206042709.\n",
      "[I 2025-12-30 23:07:28,088] Trial 8 finished with value: 0.8475151159633662 and parameters: {'max_depth': 23, 'min_samples_split': 18, 'min_samples_leaf': 9}. Best is trial 5 with value: 0.8530964206042709.\n",
      "[I 2025-12-30 23:07:29,206] Trial 9 finished with value: 0.8464990675988854 and parameters: {'max_depth': 29, 'min_samples_split': 12, 'min_samples_leaf': 8}. Best is trial 5 with value: 0.8530964206042709.\n",
      "[I 2025-12-30 23:07:30,221] Trial 10 finished with value: 0.8530451396262387 and parameters: {'max_depth': 17, 'min_samples_split': 8, 'min_samples_leaf': 6}. Best is trial 5 with value: 0.8530964206042709.\n",
      "[I 2025-12-30 23:07:31,285] Trial 11 finished with value: 0.8530451396262387 and parameters: {'max_depth': 17, 'min_samples_split': 6, 'min_samples_leaf': 6}. Best is trial 5 with value: 0.8530964206042709.\n",
      "[I 2025-12-30 23:07:32,329] Trial 12 finished with value: 0.8552688869048892 and parameters: {'max_depth': 17, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 12 with value: 0.8552688869048892.\n",
      "[I 2025-12-30 23:07:33,272] Trial 13 finished with value: 0.8174775317154911 and parameters: {'max_depth': 13, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 12 with value: 0.8552688869048892.\n",
      "[I 2025-12-30 23:07:34,379] Trial 14 finished with value: 0.8426560180402719 and parameters: {'max_depth': 21, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 12 with value: 0.8552688869048892.\n",
      "[I 2025-12-30 23:07:35,445] Trial 15 finished with value: 0.8678740954394554 and parameters: {'max_depth': 19, 'min_samples_split': 4, 'min_samples_leaf': 5}. Best is trial 15 with value: 0.8678740954394554.\n",
      "[I 2025-12-30 23:07:36,477] Trial 16 finished with value: 0.8422541099723464 and parameters: {'max_depth': 18, 'min_samples_split': 4, 'min_samples_leaf': 7}. Best is trial 15 with value: 0.8678740954394554.\n",
      "[I 2025-12-30 23:07:37,374] Trial 17 finished with value: 0.8172909285516853 and parameters: {'max_depth': 12, 'min_samples_split': 5, 'min_samples_leaf': 5}. Best is trial 15 with value: 0.8678740954394554.\n",
      "[I 2025-12-30 23:07:38,318] Trial 18 finished with value: 0.8260391896503446 and parameters: {'max_depth': 14, 'min_samples_split': 2, 'min_samples_leaf': 5}. Best is trial 15 with value: 0.8678740954394554.\n",
      "[I 2025-12-30 23:07:39,386] Trial 19 finished with value: 0.842151304547308 and parameters: {'max_depth': 20, 'min_samples_split': 5, 'min_samples_leaf': 7}. Best is trial 15 with value: 0.8678740954394554.\n",
      "[I 2025-12-30 23:07:40,068] Trial 20 finished with value: 0.7932035483076417 and parameters: {'max_depth': 7, 'min_samples_split': 11, 'min_samples_leaf': 10}. Best is trial 15 with value: 0.8678740954394554.\n",
      "[I 2025-12-30 23:07:41,185] Trial 21 finished with value: 0.852366716437747 and parameters: {'max_depth': 24, 'min_samples_split': 15, 'min_samples_leaf': 3}. Best is trial 15 with value: 0.8678740954394554.\n",
      "[I 2025-12-30 23:07:42,330] Trial 22 finished with value: 0.8514882193745293 and parameters: {'max_depth': 26, 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 15 with value: 0.8678740954394554.\n",
      "[I 2025-12-30 23:07:43,400] Trial 23 finished with value: 0.8731513733308232 and parameters: {'max_depth': 19, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 23 with value: 0.8731513733308232.\n",
      "[I 2025-12-30 23:07:44,469] Trial 24 finished with value: 0.8740803247062181 and parameters: {'max_depth': 19, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 24 with value: 0.8740803247062181.\n",
      "[I 2025-12-30 23:07:45,539] Trial 25 finished with value: 0.8740803247062181 and parameters: {'max_depth': 19, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 24 with value: 0.8740803247062181.\n",
      "[I 2025-12-30 23:07:46,526] Trial 26 finished with value: 0.8321796029110896 and parameters: {'max_depth': 15, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 24 with value: 0.8740803247062181.\n",
      "[I 2025-12-30 23:07:47,636] Trial 27 finished with value: 0.8486727423361312 and parameters: {'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 24 with value: 0.8740803247062181.\n",
      "[I 2025-12-30 23:07:48,501] Trial 28 finished with value: 0.7990204753561468 and parameters: {'max_depth': 11, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 24 with value: 0.8740803247062181.\n",
      "[I 2025-12-30 23:07:48,995] Trial 29 finished with value: 0.8652851687498854 and parameters: {'max_depth': 3, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 24 with value: 0.8740803247062181.\n",
      "[I 2025-12-30 23:07:51,310] A new study created in memory with name: no-name-9f6cef1d-0b23-41f3-ab5b-623ea1b2e38a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULTS FOR: DecisionTree\n",
      "----------------------------------------\n",
      "ACCURACY     : 0.8534\n",
      "PRECISION    : 0.8075\n",
      "RECALL       : 0.8007\n",
      "F1 SCORE     : 0.8041\n",
      "K-FOLD MEAN  : 0.8567\n",
      "K-FOLD STD   : 0.0018\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "ðŸ”¥ NOW TUNING: GradientBoosting\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-30 23:08:21,744] Trial 0 finished with value: 0.8270123236513661 and parameters: {'n_estimators': 116, 'learning_rate': 0.03016599729369588, 'max_depth': 5}. Best is trial 0 with value: 0.8270123236513661.\n",
      "[I 2025-12-30 23:09:34,283] Trial 1 finished with value: 0.8455846857662355 and parameters: {'n_estimators': 342, 'learning_rate': 0.025502399732508767, 'max_depth': 4}. Best is trial 1 with value: 0.8455846857662355.\n",
      "[I 2025-12-30 23:11:41,726] Trial 2 finished with value: 0.8794880137776798 and parameters: {'n_estimators': 490, 'learning_rate': 0.04405809692225874, 'max_depth': 5}. Best is trial 2 with value: 0.8794880137776798.\n",
      "[I 2025-12-30 23:12:50,554] Trial 3 finished with value: 0.8833714523821875 and parameters: {'n_estimators': 330, 'learning_rate': 0.19015827352054823, 'max_depth': 4}. Best is trial 3 with value: 0.8833714523821875.\n",
      "[I 2025-12-30 23:13:50,216] Trial 4 finished with value: 0.8836698605942428 and parameters: {'n_estimators': 285, 'learning_rate': 0.18331384943655002, 'max_depth': 4}. Best is trial 4 with value: 0.8836698605942428.\n",
      "[I 2025-12-30 23:15:28,029] Trial 5 finished with value: 0.8751580500044559 and parameters: {'n_estimators': 307, 'learning_rate': 0.0363159964956086, 'max_depth': 6}. Best is trial 4 with value: 0.8836698605942428.\n",
      "[I 2025-12-30 23:16:14,642] Trial 6 finished with value: 0.7848132531690487 and parameters: {'n_estimators': 441, 'learning_rate': 0.010097152610978627, 'max_depth': 2}. Best is trial 4 with value: 0.8836698605942428.\n",
      "[I 2025-12-30 23:17:03,679] Trial 7 finished with value: 0.8757209253482561 and parameters: {'n_estimators': 312, 'learning_rate': 0.1617251086258115, 'max_depth': 3}. Best is trial 4 with value: 0.8836698605942428.\n",
      "[I 2025-12-30 23:17:43,670] Trial 8 finished with value: 0.8393919915034215 and parameters: {'n_estimators': 150, 'learning_rate': 0.03300988066844032, 'max_depth': 5}. Best is trial 4 with value: 0.8836698605942428.\n",
      "[I 2025-12-30 23:18:09,672] Trial 9 finished with value: 0.8384129887010535 and parameters: {'n_estimators': 120, 'learning_rate': 0.056446660926842286, 'max_depth': 4}. Best is trial 4 with value: 0.8836698605942428.\n",
      "[I 2025-12-30 23:18:43,767] Trial 10 finished with value: 0.8597741873410213 and parameters: {'n_estimators': 212, 'learning_rate': 0.10521662394146117, 'max_depth': 3}. Best is trial 4 with value: 0.8836698605942428.\n",
      "[I 2025-12-30 23:19:33,242] Trial 11 finished with value: 0.8817789240722439 and parameters: {'n_estimators': 233, 'learning_rate': 0.18698198981002248, 'max_depth': 4}. Best is trial 4 with value: 0.8836698605942428.\n",
      "[I 2025-12-30 23:20:34,097] Trial 12 finished with value: 0.8713901202628849 and parameters: {'n_estimators': 382, 'learning_rate': 0.09195859754263747, 'max_depth': 3}. Best is trial 4 with value: 0.8836698605942428.\n",
      "[I 2025-12-30 23:20:58,770] Trial 13 finished with value: 0.8481393347357634 and parameters: {'n_estimators': 232, 'learning_rate': 0.11521033391384526, 'max_depth': 2}. Best is trial 4 with value: 0.8836698605942428.\n",
      "[I 2025-12-30 23:22:57,398] Trial 14 finished with value: 0.8863419446551348 and parameters: {'n_estimators': 380, 'learning_rate': 0.07176240293369586, 'max_depth': 6}. Best is trial 14 with value: 0.8863419446551348.\n",
      "[I 2025-12-30 23:25:06,297] Trial 15 finished with value: 0.885578231137416 and parameters: {'n_estimators': 412, 'learning_rate': 0.06562915523627433, 'max_depth': 6}. Best is trial 14 with value: 0.8863419446551348.\n",
      "[I 2025-12-30 23:27:15,402] Trial 16 finished with value: 0.8853952252403277 and parameters: {'n_estimators': 405, 'learning_rate': 0.065942067943533, 'max_depth': 6}. Best is trial 14 with value: 0.8863419446551348.\n",
      "[I 2025-12-30 23:29:49,150] Trial 17 finished with value: 0.886540572959872 and parameters: {'n_estimators': 493, 'learning_rate': 0.06987740594486673, 'max_depth': 6}. Best is trial 17 with value: 0.886540572959872.\n",
      "[I 2025-12-30 23:32:23,952] Trial 18 finished with value: 0.8877185045723436 and parameters: {'n_estimators': 496, 'learning_rate': 0.07950873494036773, 'max_depth': 6}. Best is trial 18 with value: 0.8877185045723436.\n",
      "[I 2025-12-30 23:34:30,831] Trial 19 finished with value: 0.8478900197497016 and parameters: {'n_estimators': 487, 'learning_rate': 0.013332677724322131, 'max_depth': 5}. Best is trial 18 with value: 0.8877185045723436.\n",
      "[I 2025-12-30 23:36:54,036] Trial 20 finished with value: 0.8724509513166243 and parameters: {'n_estimators': 457, 'learning_rate': 0.021504673206429157, 'max_depth': 6}. Best is trial 18 with value: 0.8877185045723436.\n",
      "[I 2025-12-30 23:39:14,969] Trial 21 finished with value: 0.8871382954479423 and parameters: {'n_estimators': 450, 'learning_rate': 0.08254852460315884, 'max_depth': 6}. Best is trial 18 with value: 0.8877185045723436.\n",
      "[I 2025-12-30 23:41:24,507] Trial 22 finished with value: 0.8883328620799924 and parameters: {'n_estimators': 497, 'learning_rate': 0.1313337877820132, 'max_depth': 5}. Best is trial 22 with value: 0.8883328620799924.\n",
      "[I 2025-12-30 23:43:20,522] Trial 23 finished with value: 0.8859596232782909 and parameters: {'n_estimators': 447, 'learning_rate': 0.1283190096544722, 'max_depth': 5}. Best is trial 22 with value: 0.8883328620799924.\n",
      "[I 2025-12-30 23:45:20,122] Trial 24 finished with value: 0.8847651342291997 and parameters: {'n_estimators': 459, 'learning_rate': 0.09135923599421086, 'max_depth': 5}. Best is trial 22 with value: 0.8883328620799924.\n",
      "[I 2025-12-30 23:47:32,969] Trial 25 finished with value: 0.8898915419976605 and parameters: {'n_estimators': 424, 'learning_rate': 0.1370827488108961, 'max_depth': 6}. Best is trial 25 with value: 0.8898915419976605.\n",
      "[I 2025-12-30 23:49:42,598] Trial 26 finished with value: 0.8869392845070195 and parameters: {'n_estimators': 500, 'learning_rate': 0.1348473499694271, 'max_depth': 5}. Best is trial 25 with value: 0.8898915419976605.\n",
      "[I 2025-12-30 23:51:51,043] Trial 27 finished with value: 0.8896106680473529 and parameters: {'n_estimators': 411, 'learning_rate': 0.14148195979018016, 'max_depth': 6}. Best is trial 25 with value: 0.8898915419976605.\n",
      "[I 2025-12-30 23:53:25,696] Trial 28 finished with value: 0.8864416326051652 and parameters: {'n_estimators': 364, 'learning_rate': 0.14611481815510344, 'max_depth': 5}. Best is trial 25 with value: 0.8898915419976605.\n",
      "[I 2025-12-30 23:55:27,625] Trial 29 finished with value: 0.8877356746057007 and parameters: {'n_estimators': 390, 'learning_rate': 0.11276542327282237, 'max_depth': 6}. Best is trial 25 with value: 0.8898915419976605.\n",
      "[I 2025-12-31 00:01:02,171] A new study created in memory with name: no-name-79be087b-c62b-4cbc-abb8-8f27a61db2bb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULTS FOR: GradientBoosting\n",
      "----------------------------------------\n",
      "ACCURACY     : 0.8805\n",
      "PRECISION    : 0.8529\n",
      "RECALL       : 0.8241\n",
      "F1 SCORE     : 0.8383\n",
      "K-FOLD MEAN  : 0.9005\n",
      "K-FOLD STD   : 0.0011\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "ðŸ”¥ NOW TUNING: Bagged DT\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-31 00:02:25,075] Trial 0 finished with value: 0.8903908742498791 and parameters: {'n_estimators': 138}. Best is trial 0 with value: 0.8903908742498791.\n",
      "[I 2025-12-31 00:04:26,393] Trial 1 finished with value: 0.892116956795529 and parameters: {'n_estimators': 203}. Best is trial 1 with value: 0.892116956795529.\n",
      "[I 2025-12-31 00:05:08,029] Trial 2 finished with value: 0.8890468281951769 and parameters: {'n_estimators': 70}. Best is trial 1 with value: 0.892116956795529.\n",
      "[I 2025-12-31 00:06:38,971] Trial 3 finished with value: 0.8921998064565102 and parameters: {'n_estimators': 153}. Best is trial 3 with value: 0.8921998064565102.\n",
      "[I 2025-12-31 00:08:01,096] Trial 4 finished with value: 0.8903908742498791 and parameters: {'n_estimators': 138}. Best is trial 3 with value: 0.8921998064565102.\n",
      "[I 2025-12-31 00:09:44,572] Trial 5 finished with value: 0.8922831974386662 and parameters: {'n_estimators': 173}. Best is trial 5 with value: 0.8922831974386662.\n",
      "[I 2025-12-31 00:12:40,257] Trial 6 finished with value: 0.8915859590310952 and parameters: {'n_estimators': 296}. Best is trial 5 with value: 0.8922831974386662.\n",
      "[I 2025-12-31 00:14:23,216] Trial 7 finished with value: 0.8922831974386662 and parameters: {'n_estimators': 173}. Best is trial 5 with value: 0.8922831974386662.\n",
      "[I 2025-12-31 00:17:54,016] Trial 8 finished with value: 0.8919673395859661 and parameters: {'n_estimators': 358}. Best is trial 5 with value: 0.8922831974386662.\n",
      "[I 2025-12-31 00:20:48,293] Trial 9 finished with value: 0.8915527799204375 and parameters: {'n_estimators': 294}. Best is trial 5 with value: 0.8922831974386662.\n",
      "[I 2025-12-31 00:21:21,747] Trial 10 finished with value: 0.8884168472510906 and parameters: {'n_estimators': 56}. Best is trial 5 with value: 0.8922831974386662.\n",
      "[I 2025-12-31 00:23:32,472] Trial 11 finished with value: 0.8923163632037837 and parameters: {'n_estimators': 219}. Best is trial 11 with value: 0.8923163632037837.\n",
      "[I 2025-12-31 00:25:56,965] Trial 12 finished with value: 0.8925816895410765 and parameters: {'n_estimators': 243}. Best is trial 12 with value: 0.8925816895410765.\n",
      "[I 2025-12-31 00:28:29,158] Trial 13 finished with value: 0.8922997745282228 and parameters: {'n_estimators': 257}. Best is trial 12 with value: 0.8925816895410765.\n",
      "[I 2025-12-31 00:30:56,088] Trial 14 finished with value: 0.891668795346536 and parameters: {'n_estimators': 248}. Best is trial 12 with value: 0.8925816895410765.\n",
      "[I 2025-12-31 00:34:10,756] Trial 15 finished with value: 0.8917850938989943 and parameters: {'n_estimators': 328}. Best is trial 12 with value: 0.8925816895410765.\n",
      "[I 2025-12-31 00:38:06,771] Trial 16 finished with value: 0.8919674385813975 and parameters: {'n_estimators': 398}. Best is trial 12 with value: 0.8925816895410765.\n",
      "[I 2025-12-31 00:40:20,751] Trial 17 finished with value: 0.8923497808563199 and parameters: {'n_estimators': 225}. Best is trial 12 with value: 0.8925816895410765.\n",
      "[I 2025-12-31 00:43:02,338] Trial 18 finished with value: 0.8922328581211199 and parameters: {'n_estimators': 273}. Best is trial 12 with value: 0.8925816895410765.\n",
      "[I 2025-12-31 00:44:07,424] Trial 19 finished with value: 0.8893616044343154 and parameters: {'n_estimators': 108}. Best is trial 12 with value: 0.8925816895410765.\n",
      "[I 2025-12-31 00:46:07,953] Trial 20 finished with value: 0.892116956795529 and parameters: {'n_estimators': 203}. Best is trial 12 with value: 0.8925816895410765.\n",
      "[I 2025-12-31 00:48:20,651] Trial 21 finished with value: 0.8921670638785937 and parameters: {'n_estimators': 223}. Best is trial 12 with value: 0.8925816895410765.\n",
      "[I 2025-12-31 00:50:35,313] Trial 22 finished with value: 0.8914535430938305 and parameters: {'n_estimators': 226}. Best is trial 12 with value: 0.8925816895410765.\n",
      "[I 2025-12-31 00:52:50,303] Trial 23 finished with value: 0.892382855178544 and parameters: {'n_estimators': 227}. Best is trial 12 with value: 0.8925816895410765.\n",
      "[I 2025-12-31 00:55:40,562] Trial 24 finished with value: 0.8922000936163363 and parameters: {'n_estimators': 287}. Best is trial 12 with value: 0.8925816895410765.\n",
      "[I 2025-12-31 00:58:55,466] Trial 25 finished with value: 0.8917850938989943 and parameters: {'n_estimators': 328}. Best is trial 12 with value: 0.8925816895410765.\n",
      "[I 2025-12-31 01:01:25,483] Trial 26 finished with value: 0.8925153990762512 and parameters: {'n_estimators': 253}. Best is trial 12 with value: 0.8925816895410765.\n",
      "[I 2025-12-31 01:03:58,055] Trial 27 finished with value: 0.8915528418839255 and parameters: {'n_estimators': 256}. Best is trial 12 with value: 0.8925816895410765.\n",
      "[I 2025-12-31 01:05:51,833] Trial 28 finished with value: 0.8908226936321221 and parameters: {'n_estimators': 192}. Best is trial 12 with value: 0.8925816895410765.\n",
      "[I 2025-12-31 01:09:05,170] Trial 29 finished with value: 0.8924983801753456 and parameters: {'n_estimators': 325}. Best is trial 12 with value: 0.8925816895410765.\n",
      "[I 2025-12-31 01:15:13,764] A new study created in memory with name: no-name-3604c4fc-bcf8-469e-8425-2eb75156bf81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULTS FOR: Bagged DT\n",
      "----------------------------------------\n",
      "ACCURACY     : 0.8805\n",
      "PRECISION    : 0.8460\n",
      "RECALL       : 0.8337\n",
      "F1 SCORE     : 0.8398\n",
      "K-FOLD MEAN  : 0.8982\n",
      "K-FOLD STD   : 0.0012\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "ðŸ”¥ NOW TUNING: SVC\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-31 01:41:47,568] Trial 0 finished with value: 0.7902904029664232 and parameters: {'C': 5.034250967382219, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 0 with value: 0.7902904029664232.\n",
      "[I 2025-12-31 01:58:40,180] Trial 1 finished with value: 0.7812155104606466 and parameters: {'C': 0.8414813733793879, 'kernel': 'linear', 'gamma': 'auto'}. Best is trial 0 with value: 0.7902904029664232.\n",
      "[I 2025-12-31 02:25:45,466] Trial 2 finished with value: 0.792464718751023 and parameters: {'C': 0.14060422212685975, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 2 with value: 0.792464718751023.\n",
      "[I 2025-12-31 02:44:43,638] Trial 3 finished with value: 0.763805252521593 and parameters: {'C': 0.01080904344058056, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 2 with value: 0.792464718751023.\n",
      "[I 2025-12-31 03:01:38,038] Trial 4 finished with value: 0.7769156033411894 and parameters: {'C': 0.048471118316184204, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 2 with value: 0.792464718751023.\n",
      "[I 2025-12-31 03:34:10,445] Trial 5 finished with value: 0.7810981783981802 and parameters: {'C': 8.824833044276069, 'kernel': 'linear', 'gamma': 'auto'}. Best is trial 2 with value: 0.792464718751023.\n",
      "[I 2025-12-31 04:00:08,340] Trial 6 finished with value: 0.7995668387145379 and parameters: {'C': 1.4455012032811783, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 6 with value: 0.7995668387145379.\n",
      "[I 2025-12-31 04:35:15,497] Trial 7 finished with value: 0.7404249024881366 and parameters: {'C': 0.04841994728540091, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 6 with value: 0.7995668387145379.\n",
      "[I 2025-12-31 05:03:17,457] Trial 8 finished with value: 0.8168908714031589 and parameters: {'C': 5.700990881975128, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 8 with value: 0.8168908714031589.\n",
      "[I 2025-12-31 05:35:54,514] Trial 9 finished with value: 0.7587268870901109 and parameters: {'C': 0.15751626945290143, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 8 with value: 0.8168908714031589.\n",
      "[I 2025-12-31 06:01:08,402] Trial 10 finished with value: 0.8027692842567236 and parameters: {'C': 1.8881422899505638, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 8 with value: 0.8168908714031589.\n",
      "[I 2025-12-31 06:26:33,101] Trial 11 finished with value: 0.8047107407081251 and parameters: {'C': 2.173163614150881, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 8 with value: 0.8168908714031589.\n",
      "[I 2025-12-31 06:53:28,755] Trial 12 finished with value: 0.8083778043809562 and parameters: {'C': 3.0000613365455853, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 8 with value: 0.8168908714031589.\n",
      "[I 2025-12-31 07:18:56,708] Trial 13 finished with value: 0.7894109488656801 and parameters: {'C': 0.5033478641285849, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 8 with value: 0.8168908714031589.\n",
      "[I 2025-12-31 07:47:01,917] Trial 14 finished with value: 0.8141197311275162 and parameters: {'C': 4.603879973060858, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 8 with value: 0.8168908714031589.\n",
      "[I 2025-12-31 08:18:15,948] Trial 15 finished with value: 0.8238601640445656 and parameters: {'C': 9.417972941784013, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 15 with value: 0.8238601640445656.\n",
      "[I 2025-12-31 08:49:43,478] Trial 16 finished with value: 0.8234786381502458 and parameters: {'C': 9.165025273573278, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 15 with value: 0.8238601640445656.\n",
      "[I 2025-12-31 09:20:48,459] Trial 17 finished with value: 0.8232629806037401 and parameters: {'C': 9.059035567579492, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 15 with value: 0.8238601640445656.\n",
      "[I 2025-12-31 09:37:44,542] Trial 18 finished with value: 0.7811988036511117 and parameters: {'C': 0.8990522956574778, 'kernel': 'linear', 'gamma': 'scale'}. Best is trial 15 with value: 0.8238601640445656.\n",
      "[I 2025-12-31 10:04:58,817] Trial 19 finished with value: 0.7906228349040023 and parameters: {'C': 0.2588222317752025, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 15 with value: 0.8238601640445656.\n",
      "[I 2025-12-31 10:32:13,835] Trial 20 finished with value: 0.8132567714711162 and parameters: {'C': 4.242112996735572, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 15 with value: 0.8238601640445656.\n",
      "[I 2025-12-31 11:02:45,660] Trial 21 finished with value: 0.8214379078402937 and parameters: {'C': 7.9126220890326415, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 15 with value: 0.8238601640445656.\n",
      "[I 2025-12-31 11:29:29,224] Trial 22 finished with value: 0.8075148893090377 and parameters: {'C': 2.8163323126118844, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 15 with value: 0.8238601640445656.\n",
      "[I 2025-12-31 12:00:10,445] Trial 23 finished with value: 0.8230803180369639 and parameters: {'C': 8.959172530199048, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 15 with value: 0.8238601640445656.\n",
      "[I 2025-12-31 12:32:20,384] Trial 24 finished with value: 0.8238436025744796 and parameters: {'C': 9.390383189335132, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 15 with value: 0.8238601640445656.\n",
      "[I 2025-12-31 12:58:23,936] Trial 25 finished with value: 0.7961989996343646 and parameters: {'C': 1.150068718510431, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 15 with value: 0.8238601640445656.\n",
      "[I 2025-12-31 13:20:04,163] Trial 26 finished with value: 0.781546181162184 and parameters: {'C': 3.5273948953632686, 'kernel': 'linear', 'gamma': 'auto'}. Best is trial 15 with value: 0.8238601640445656.\n",
      "[I 2025-12-31 13:46:15,162] Trial 27 finished with value: 0.7901592258662143 and parameters: {'C': 0.610500077774676, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 15 with value: 0.8238601640445656.\n",
      "[I 2025-12-31 14:15:01,741] Trial 28 finished with value: 0.8167913068657106 and parameters: {'C': 5.6762042793766065, 'kernel': 'rbf', 'gamma': 'scale'}. Best is trial 15 with value: 0.8238601640445656.\n",
      "[I 2025-12-31 14:41:15,948] Trial 29 finished with value: 0.790074784211397 and parameters: {'C': 5.517697812595903, 'kernel': 'rbf', 'gamma': 'auto'}. Best is trial 15 with value: 0.8238601640445656.\n",
      "[I 2025-12-31 16:11:03,109] A new study created in memory with name: no-name-32b94c27-c3fe-4c2f-bd62-d92c82f14e07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULTS FOR: SVC\n",
      "----------------------------------------\n",
      "ACCURACY     : 0.8353\n",
      "PRECISION    : 0.7664\n",
      "RECALL       : 0.8079\n",
      "F1 SCORE     : 0.7866\n",
      "K-FOLD MEAN  : 0.8362\n",
      "K-FOLD STD   : 0.0006\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "ðŸ”¥ NOW TUNING: KNN\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-31 16:11:15,049] Trial 0 finished with value: 0.8992863414770259 and parameters: {'n_neighbors': 4, 'weights': 'distance', 'p': 2}. Best is trial 0 with value: 0.8992863414770259.\n",
      "[I 2025-12-31 16:11:59,949] Trial 1 finished with value: 0.8078455793654707 and parameters: {'n_neighbors': 6, 'weights': 'uniform', 'p': 1}. Best is trial 0 with value: 0.8992863414770259.\n",
      "[I 2025-12-31 16:12:12,750] Trial 2 finished with value: 0.830450134786144 and parameters: {'n_neighbors': 29, 'weights': 'uniform', 'p': 2}. Best is trial 0 with value: 0.8992863414770259.\n",
      "[I 2025-12-31 16:12:25,346] Trial 3 finished with value: 0.8865916281194068 and parameters: {'n_neighbors': 23, 'weights': 'distance', 'p': 2}. Best is trial 0 with value: 0.8992863414770259.\n",
      "[I 2025-12-31 16:13:09,753] Trial 4 finished with value: 0.9040151203406892 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'p': 1}. Best is trial 4 with value: 0.9040151203406892.\n",
      "[I 2025-12-31 16:13:22,359] Trial 5 finished with value: 0.8310643260562655 and parameters: {'n_neighbors': 25, 'weights': 'uniform', 'p': 2}. Best is trial 4 with value: 0.9040151203406892.\n",
      "[I 2025-12-31 16:13:34,714] Trial 6 finished with value: 0.7986051711542661 and parameters: {'n_neighbors': 10, 'weights': 'uniform', 'p': 2}. Best is trial 4 with value: 0.9040151203406892.\n",
      "[I 2025-12-31 16:13:46,333] Trial 7 finished with value: 0.8727177451246954 and parameters: {'n_neighbors': 3, 'weights': 'uniform', 'p': 2}. Best is trial 4 with value: 0.9040151203406892.\n",
      "[I 2025-12-31 16:13:58,183] Trial 8 finished with value: 0.8839360400460405 and parameters: {'n_neighbors': 30, 'weights': 'distance', 'p': 2}. Best is trial 4 with value: 0.9040151203406892.\n",
      "[I 2025-12-31 16:14:42,696] Trial 9 finished with value: 0.902555362884327 and parameters: {'n_neighbors': 21, 'weights': 'distance', 'p': 1}. Best is trial 4 with value: 0.9040151203406892.\n",
      "[I 2025-12-31 16:15:26,320] Trial 10 finished with value: 0.903998283811176 and parameters: {'n_neighbors': 15, 'weights': 'distance', 'p': 1}. Best is trial 4 with value: 0.9040151203406892.\n",
      "[I 2025-12-31 16:16:10,028] Trial 11 finished with value: 0.9035836679751613 and parameters: {'n_neighbors': 14, 'weights': 'distance', 'p': 1}. Best is trial 4 with value: 0.9040151203406892.\n",
      "[I 2025-12-31 16:16:54,481] Trial 12 finished with value: 0.9036494853896961 and parameters: {'n_neighbors': 16, 'weights': 'distance', 'p': 1}. Best is trial 4 with value: 0.9040151203406892.\n",
      "[I 2025-12-31 16:17:39,047] Trial 13 finished with value: 0.9054918173554626 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'p': 1}. Best is trial 13 with value: 0.9054918173554626.\n",
      "[I 2025-12-31 16:18:23,016] Trial 14 finished with value: 0.9054918173554626 and parameters: {'n_neighbors': 11, 'weights': 'distance', 'p': 1}. Best is trial 13 with value: 0.9054918173554626.\n",
      "[I 2025-12-31 16:19:07,088] Trial 15 finished with value: 0.9049606430126386 and parameters: {'n_neighbors': 10, 'weights': 'distance', 'p': 1}. Best is trial 13 with value: 0.9054918173554626.\n",
      "[I 2025-12-31 16:19:51,087] Trial 16 finished with value: 0.904745039877083 and parameters: {'n_neighbors': 8, 'weights': 'distance', 'p': 1}. Best is trial 13 with value: 0.9054918173554626.\n",
      "[I 2025-12-31 16:20:35,641] Trial 17 finished with value: 0.9033346310532046 and parameters: {'n_neighbors': 19, 'weights': 'distance', 'p': 1}. Best is trial 13 with value: 0.9054918173554626.\n",
      "[I 2025-12-31 16:21:19,915] Trial 18 finished with value: 0.9033507622979456 and parameters: {'n_neighbors': 18, 'weights': 'distance', 'p': 1}. Best is trial 13 with value: 0.9054918173554626.\n",
      "[I 2025-12-31 16:22:03,374] Trial 19 finished with value: 0.9040151203406892 and parameters: {'n_neighbors': 12, 'weights': 'distance', 'p': 1}. Best is trial 13 with value: 0.9054918173554626.\n",
      "[I 2025-12-31 16:22:47,471] Trial 20 finished with value: 0.905590375588378 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'p': 1}. Best is trial 20 with value: 0.905590375588378.\n",
      "[I 2025-12-31 16:23:31,891] Trial 21 finished with value: 0.905590375588378 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'p': 1}. Best is trial 20 with value: 0.905590375588378.\n",
      "[I 2025-12-31 16:24:15,910] Trial 22 finished with value: 0.905590375588378 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'p': 1}. Best is trial 20 with value: 0.905590375588378.\n",
      "[I 2025-12-31 16:24:59,914] Trial 23 finished with value: 0.905590375588378 and parameters: {'n_neighbors': 7, 'weights': 'distance', 'p': 1}. Best is trial 20 with value: 0.905590375588378.\n",
      "[I 2025-12-31 16:25:44,039] Trial 24 finished with value: 0.9056414154264832 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'p': 1}. Best is trial 24 with value: 0.9056414154264832.\n",
      "[I 2025-12-31 16:26:28,148] Trial 25 finished with value: 0.9056414154264832 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'p': 1}. Best is trial 24 with value: 0.9056414154264832.\n",
      "[I 2025-12-31 16:27:11,925] Trial 26 finished with value: 0.805123449968649 and parameters: {'n_neighbors': 4, 'weights': 'uniform', 'p': 1}. Best is trial 24 with value: 0.9056414154264832.\n",
      "[I 2025-12-31 16:27:55,401] Trial 27 finished with value: 0.9056414154264832 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'p': 1}. Best is trial 24 with value: 0.9056414154264832.\n",
      "[I 2025-12-31 16:28:39,384] Trial 28 finished with value: 0.9056414154264832 and parameters: {'n_neighbors': 5, 'weights': 'distance', 'p': 1}. Best is trial 24 with value: 0.9056414154264832.\n",
      "[I 2025-12-31 16:29:23,647] Trial 29 finished with value: 0.9040145795339085 and parameters: {'n_neighbors': 4, 'weights': 'distance', 'p': 1}. Best is trial 24 with value: 0.9056414154264832.\n",
      "[I 2025-12-31 16:30:13,487] A new study created in memory with name: no-name-a93ac2f5-4f81-4560-80ca-c449211e9993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULTS FOR: KNN\n",
      "----------------------------------------\n",
      "ACCURACY     : 0.7831\n",
      "PRECISION    : 0.6819\n",
      "RECALL       : 0.7921\n",
      "F1 SCORE     : 0.7329\n",
      "K-FOLD MEAN  : 0.8335\n",
      "K-FOLD STD   : 0.0026\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "ðŸ”¥ NOW TUNING: ExtraTrees\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-31 16:30:22,568] Trial 0 finished with value: 0.8638200156749939 and parameters: {'n_estimators': 523, 'max_depth': 20, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.8638200156749939.\n",
      "[I 2025-12-31 16:30:30,065] Trial 1 finished with value: 0.8495011077644682 and parameters: {'n_estimators': 465, 'max_depth': 22, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.8638200156749939.\n",
      "[I 2025-12-31 16:30:38,091] Trial 2 finished with value: 0.8680378068688591 and parameters: {'n_estimators': 449, 'max_depth': 26, 'min_samples_leaf': 3}. Best is trial 2 with value: 0.8680378068688591.\n",
      "[I 2025-12-31 16:30:42,904] Trial 3 finished with value: 0.8787251907044501 and parameters: {'n_estimators': 276, 'max_depth': 23, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.8787251907044501.\n",
      "[I 2025-12-31 16:30:50,315] Trial 4 finished with value: 0.8593245621566753 and parameters: {'n_estimators': 483, 'max_depth': 19, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.8787251907044501.\n",
      "[I 2025-12-31 16:30:59,333] Trial 5 finished with value: 0.8900584907138945 and parameters: {'n_estimators': 472, 'max_depth': 26, 'min_samples_leaf': 1}. Best is trial 5 with value: 0.8900584907138945.\n",
      "[I 2025-12-31 16:31:00,278] Trial 6 finished with value: 0.8200298123472013 and parameters: {'n_estimators': 148, 'max_depth': 5, 'min_samples_leaf': 1}. Best is trial 5 with value: 0.8900584907138945.\n",
      "[I 2025-12-31 16:31:09,700] Trial 7 finished with value: 0.8815628085805242 and parameters: {'n_estimators': 496, 'max_depth': 28, 'min_samples_leaf': 2}. Best is trial 5 with value: 0.8900584907138945.\n",
      "[I 2025-12-31 16:31:10,884] Trial 8 finished with value: 0.8300151986053467 and parameters: {'n_estimators': 144, 'max_depth': 8, 'min_samples_leaf': 2}. Best is trial 5 with value: 0.8900584907138945.\n",
      "[I 2025-12-31 16:31:21,638] Trial 9 finished with value: 0.8721364050380372 and parameters: {'n_estimators': 581, 'max_depth': 29, 'min_samples_leaf': 3}. Best is trial 5 with value: 0.8900584907138945.\n",
      "[I 2025-12-31 16:31:25,364] Trial 10 finished with value: 0.8316615466107219 and parameters: {'n_estimators': 336, 'max_depth': 13, 'min_samples_leaf': 5}. Best is trial 5 with value: 0.8900584907138945.\n",
      "[I 2025-12-31 16:31:32,682] Trial 11 finished with value: 0.8845654094221779 and parameters: {'n_estimators': 379, 'max_depth': 30, 'min_samples_leaf': 2}. Best is trial 5 with value: 0.8900584907138945.\n",
      "[I 2025-12-31 16:31:40,097] Trial 12 finished with value: 0.8846152333788826 and parameters: {'n_estimators': 380, 'max_depth': 30, 'min_samples_leaf': 2}. Best is trial 5 with value: 0.8900584907138945.\n",
      "[I 2025-12-31 16:31:44,618] Trial 13 finished with value: 0.841647649143562 and parameters: {'n_estimators': 366, 'max_depth': 15, 'min_samples_leaf': 3}. Best is trial 5 with value: 0.8900584907138945.\n",
      "[I 2025-12-31 16:31:48,922] Trial 14 finished with value: 0.8753053153081071 and parameters: {'n_estimators': 241, 'max_depth': 25, 'min_samples_leaf': 2}. Best is trial 5 with value: 0.8900584907138945.\n",
      "[I 2025-12-31 16:31:56,519] Trial 15 finished with value: 0.8645049117880079 and parameters: {'n_estimators': 421, 'max_depth': 30, 'min_samples_leaf': 4}. Best is trial 5 with value: 0.8900584907138945.\n",
      "[I 2025-12-31 16:32:06,982] Trial 16 finished with value: 0.8762837745153695 and parameters: {'n_estimators': 578, 'max_depth': 25, 'min_samples_leaf': 2}. Best is trial 5 with value: 0.8900584907138945.\n",
      "[I 2025-12-31 16:32:09,983] Trial 17 finished with value: 0.831180734757453 and parameters: {'n_estimators': 291, 'max_depth': 12, 'min_samples_leaf': 1}. Best is trial 5 with value: 0.8900584907138945.\n",
      "[I 2025-12-31 16:32:15,579] Trial 18 finished with value: 0.8450854683452724 and parameters: {'n_estimators': 391, 'max_depth': 18, 'min_samples_leaf': 4}. Best is trial 5 with value: 0.8900584907138945.\n",
      "[I 2025-12-31 16:32:19,472] Trial 19 finished with value: 0.8805172563877685 and parameters: {'n_estimators': 210, 'max_depth': 27, 'min_samples_leaf': 2}. Best is trial 5 with value: 0.8900584907138945.\n",
      "[I 2025-12-31 16:32:24,767] Trial 20 finished with value: 0.8552766136368782 and parameters: {'n_estimators': 323, 'max_depth': 23, 'min_samples_leaf': 4}. Best is trial 5 with value: 0.8900584907138945.\n",
      "[I 2025-12-31 16:32:32,775] Trial 21 finished with value: 0.88466507874806 and parameters: {'n_estimators': 409, 'max_depth': 30, 'min_samples_leaf': 2}. Best is trial 5 with value: 0.8900584907138945.\n",
      "[I 2025-12-31 16:32:40,938] Trial 22 finished with value: 0.8846650069581035 and parameters: {'n_estimators': 415, 'max_depth': 30, 'min_samples_leaf': 2}. Best is trial 5 with value: 0.8900584907138945.\n",
      "[I 2025-12-31 16:32:50,595] Trial 23 finished with value: 0.8695146960814935 and parameters: {'n_estimators': 522, 'max_depth': 27, 'min_samples_leaf': 3}. Best is trial 5 with value: 0.8900584907138945.\n",
      "[I 2025-12-31 16:32:58,618] Trial 24 finished with value: 0.8841344323808706 and parameters: {'n_estimators': 424, 'max_depth': 24, 'min_samples_leaf': 1}. Best is trial 5 with value: 0.8900584907138945.\n",
      "[I 2025-12-31 16:33:07,347] Trial 25 finished with value: 0.8937764118534384 and parameters: {'n_estimators': 426, 'max_depth': 28, 'min_samples_leaf': 1}. Best is trial 25 with value: 0.8937764118534384.\n",
      "[I 2025-12-31 16:33:16,991] Trial 26 finished with value: 0.8744251949304239 and parameters: {'n_estimators': 540, 'max_depth': 22, 'min_samples_leaf': 1}. Best is trial 25 with value: 0.8937764118534384.\n",
      "[I 2025-12-31 16:33:26,006] Trial 27 finished with value: 0.8922157691897934 and parameters: {'n_estimators': 450, 'max_depth': 27, 'min_samples_leaf': 1}. Best is trial 25 with value: 0.8937764118534384.\n",
      "[I 2025-12-31 16:33:32,144] Trial 28 finished with value: 0.8480563976653553 and parameters: {'n_estimators': 446, 'max_depth': 16, 'min_samples_leaf': 1}. Best is trial 25 with value: 0.8937764118534384.\n",
      "[I 2025-12-31 16:33:40,845] Trial 29 finished with value: 0.8636376634400524 and parameters: {'n_estimators': 537, 'max_depth': 20, 'min_samples_leaf': 1}. Best is trial 25 with value: 0.8937764118534384.\n",
      "[I 2025-12-31 16:33:53,925] A new study created in memory with name: no-name-f916cbc0-0788-40e2-94cc-2439e28971bd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULTS FOR: ExtraTrees\n",
      "----------------------------------------\n",
      "ACCURACY     : 0.8728\n",
      "PRECISION    : 0.8421\n",
      "RECALL       : 0.8141\n",
      "F1 SCORE     : 0.8279\n",
      "K-FOLD MEAN  : 0.8974\n",
      "K-FOLD STD   : 0.0001\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "ðŸ”¥ NOW TUNING: Passive Aggressive\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-31 16:33:54,430] Trial 0 finished with value: 0.6989327658861507 and parameters: {'C': 3.403565051258518, 'loss': 'squared_hinge'}. Best is trial 0 with value: 0.6989327658861507.\n",
      "[I 2025-12-31 16:33:55,021] Trial 1 finished with value: 0.7933550766765052 and parameters: {'C': 0.1930321065237745, 'loss': 'hinge'}. Best is trial 1 with value: 0.7933550766765052.\n",
      "[I 2025-12-31 16:33:55,485] Trial 2 finished with value: 0.7037606561075783 and parameters: {'C': 0.25549667616041055, 'loss': 'squared_hinge'}. Best is trial 1 with value: 0.7933550766765052.\n",
      "[I 2025-12-31 16:33:55,950] Trial 3 finished with value: 0.6988996394269069 and parameters: {'C': 4.718218389826657, 'loss': 'hinge'}. Best is trial 1 with value: 0.7933550766765052.\n",
      "[I 2025-12-31 16:33:56,416] Trial 4 finished with value: 0.699032471729582 and parameters: {'C': 2.1814821783451284, 'loss': 'squared_hinge'}. Best is trial 1 with value: 0.7933550766765052.\n",
      "[I 2025-12-31 16:33:56,894] Trial 5 finished with value: 0.6988996394269069 and parameters: {'C': 9.82392442761084, 'loss': 'hinge'}. Best is trial 1 with value: 0.7933550766765052.\n",
      "[I 2025-12-31 16:33:57,447] Trial 6 finished with value: 0.8065878287043229 and parameters: {'C': 0.023345594109415363, 'loss': 'squared_hinge'}. Best is trial 6 with value: 0.8065878287043229.\n",
      "[I 2025-12-31 16:33:57,956] Trial 7 finished with value: 0.770195954783568 and parameters: {'C': 0.04935809312654517, 'loss': 'hinge'}. Best is trial 6 with value: 0.8065878287043229.\n",
      "[I 2025-12-31 16:33:58,479] Trial 8 finished with value: 0.7675023530389531 and parameters: {'C': 0.046554778623960326, 'loss': 'hinge'}. Best is trial 6 with value: 0.8065878287043229.\n",
      "[I 2025-12-31 16:33:58,950] Trial 9 finished with value: 0.7071968396250937 and parameters: {'C': 0.5085575011298445, 'loss': 'hinge'}. Best is trial 6 with value: 0.8065878287043229.\n",
      "[I 2025-12-31 16:33:59,515] Trial 10 finished with value: 0.7449021921376694 and parameters: {'C': 0.010272822913951011, 'loss': 'squared_hinge'}. Best is trial 6 with value: 0.8065878287043229.\n",
      "[I 2025-12-31 16:33:59,993] Trial 11 finished with value: 0.7125952397729375 and parameters: {'C': 0.09859161109472019, 'loss': 'squared_hinge'}. Best is trial 6 with value: 0.8065878287043229.\n",
      "[I 2025-12-31 16:34:00,565] Trial 12 finished with value: 0.8076501517276787 and parameters: {'C': 0.02448049891289773, 'loss': 'squared_hinge'}. Best is trial 12 with value: 0.8076501517276787.\n",
      "[I 2025-12-31 16:34:01,138] Trial 13 finished with value: 0.74505248970046 and parameters: {'C': 0.010147022270351209, 'loss': 'squared_hinge'}. Best is trial 12 with value: 0.8076501517276787.\n",
      "[I 2025-12-31 16:34:01,700] Trial 14 finished with value: 0.8102561869519894 and parameters: {'C': 0.027342046830977698, 'loss': 'squared_hinge'}. Best is trial 14 with value: 0.8102561869519894.\n",
      "[I 2025-12-31 16:34:02,271] Trial 15 finished with value: 0.8124140869039932 and parameters: {'C': 0.02984162070702094, 'loss': 'squared_hinge'}. Best is trial 15 with value: 0.8124140869039932.\n",
      "[I 2025-12-31 16:34:02,721] Trial 16 finished with value: 0.6996477944763543 and parameters: {'C': 0.8581134116060385, 'loss': 'squared_hinge'}. Best is trial 15 with value: 0.8124140869039932.\n",
      "[I 2025-12-31 16:34:03,286] Trial 17 finished with value: 0.8084894305580468 and parameters: {'C': 0.0873386370758462, 'loss': 'squared_hinge'}. Best is trial 15 with value: 0.8124140869039932.\n",
      "[I 2025-12-31 16:34:03,840] Trial 18 finished with value: 0.8051437372113025 and parameters: {'C': 0.02221495785249672, 'loss': 'squared_hinge'}. Best is trial 15 with value: 0.8124140869039932.\n",
      "[I 2025-12-31 16:34:04,302] Trial 19 finished with value: 0.7099750169930078 and parameters: {'C': 0.11402648207352353, 'loss': 'squared_hinge'}. Best is trial 15 with value: 0.8124140869039932.\n",
      "[I 2025-12-31 16:34:04,828] Trial 20 finished with value: 0.8127388381573694 and parameters: {'C': 0.042345768800322876, 'loss': 'squared_hinge'}. Best is trial 20 with value: 0.8127388381573694.\n",
      "[I 2025-12-31 16:34:05,370] Trial 21 finished with value: 0.8126388670809792 and parameters: {'C': 0.04314421680432567, 'loss': 'squared_hinge'}. Best is trial 20 with value: 0.8127388381573694.\n",
      "[I 2025-12-31 16:34:05,909] Trial 22 finished with value: 0.8126388728739813 and parameters: {'C': 0.04312102040299024, 'loss': 'squared_hinge'}. Best is trial 20 with value: 0.8127388381573694.\n",
      "[I 2025-12-31 16:34:06,443] Trial 23 finished with value: 0.8100205457070137 and parameters: {'C': 0.07377494607841155, 'loss': 'squared_hinge'}. Best is trial 20 with value: 0.8127388381573694.\n",
      "[I 2025-12-31 16:34:06,889] Trial 24 finished with value: 0.7062112807646734 and parameters: {'C': 0.14380304408380265, 'loss': 'squared_hinge'}. Best is trial 20 with value: 0.8127388381573694.\n",
      "[I 2025-12-31 16:34:07,348] Trial 25 finished with value: 0.7094685513597233 and parameters: {'C': 0.4817822741087355, 'loss': 'squared_hinge'}. Best is trial 20 with value: 0.8127388381573694.\n",
      "[I 2025-12-31 16:34:07,896] Trial 26 finished with value: 0.812318665991108 and parameters: {'C': 0.0525210317546958, 'loss': 'squared_hinge'}. Best is trial 20 with value: 0.8127388381573694.\n",
      "[I 2025-12-31 16:34:08,449] Trial 27 finished with value: 0.8035601519456453 and parameters: {'C': 0.015040599688489215, 'loss': 'squared_hinge'}. Best is trial 20 with value: 0.8127388381573694.\n",
      "[I 2025-12-31 16:34:08,990] Trial 28 finished with value: 0.8126366862584962 and parameters: {'C': 0.04662445529905075, 'loss': 'squared_hinge'}. Best is trial 20 with value: 0.8127388381573694.\n",
      "[I 2025-12-31 16:34:09,449] Trial 29 finished with value: 0.7044046095354553 and parameters: {'C': 0.16680840328660848, 'loss': 'squared_hinge'}. Best is trial 20 with value: 0.8127388381573694.\n",
      "[I 2025-12-31 16:34:10,308] A new study created in memory with name: no-name-041c9dae-c22d-46bc-a1f8-ca7d9674305c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULTS FOR: Passive Aggressive\n",
      "----------------------------------------\n",
      "ACCURACY     : 0.7597\n",
      "PRECISION    : 0.6470\n",
      "RECALL       : 0.7929\n",
      "F1 SCORE     : 0.7126\n",
      "K-FOLD MEAN  : 0.7318\n",
      "K-FOLD STD   : 0.0205\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "ðŸ”¥ NOW TUNING: Logistic Regression\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-31 16:34:11,107] Trial 0 finished with value: 0.775388540832183 and parameters: {'C': 0.03154061670913458, 'solver': 'liblinear'}. Best is trial 0 with value: 0.775388540832183.\n",
      "[I 2025-12-31 16:34:11,874] Trial 1 finished with value: 0.7699132792025042 and parameters: {'C': 0.2725493781867354, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.775388540832183.\n",
      "[I 2025-12-31 16:34:12,608] Trial 2 finished with value: 0.770410316748085 and parameters: {'C': 0.1945841577775265, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.775388540832183.\n",
      "[I 2025-12-31 16:34:13,580] Trial 3 finished with value: 0.7716056795934065 and parameters: {'C': 2.8534171815580294, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.775388540832183.\n",
      "[I 2025-12-31 16:34:14,571] Trial 4 finished with value: 0.7714398633826121 and parameters: {'C': 3.0127919305046538, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.775388540832183.\n",
      "[I 2025-12-31 16:34:16,294] Trial 5 finished with value: 0.770974018298956 and parameters: {'C': 3.260246679930938, 'solver': 'liblinear'}. Best is trial 0 with value: 0.775388540832183.\n",
      "[I 2025-12-31 16:34:17,377] Trial 6 finished with value: 0.7709583815169783 and parameters: {'C': 2.368415813759385, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.775388540832183.\n",
      "[I 2025-12-31 16:34:19,201] Trial 7 finished with value: 0.7709407326403285 and parameters: {'C': 4.107613551563501, 'solver': 'liblinear'}. Best is trial 0 with value: 0.775388540832183.\n",
      "[I 2025-12-31 16:34:19,896] Trial 8 finished with value: 0.7702765048319576 and parameters: {'C': 0.10978978155222621, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.775388540832183.\n",
      "[I 2025-12-31 16:34:21,743] Trial 9 finished with value: 0.7710402311808227 and parameters: {'C': 6.990871084671759, 'solver': 'liblinear'}. Best is trial 0 with value: 0.775388540832183.\n",
      "[I 2025-12-31 16:34:22,385] Trial 10 finished with value: 0.7736627749257643 and parameters: {'C': 0.01298578873443345, 'solver': 'liblinear'}. Best is trial 0 with value: 0.775388540832183.\n",
      "[I 2025-12-31 16:34:23,143] Trial 11 finished with value: 0.7757538120691798 and parameters: {'C': 0.01662634002148368, 'solver': 'liblinear'}. Best is trial 11 with value: 0.7757538120691798.\n",
      "[I 2025-12-31 16:34:23,839] Trial 12 finished with value: 0.7728993740138104 and parameters: {'C': 0.011906702264428904, 'solver': 'liblinear'}. Best is trial 11 with value: 0.7757538120691798.\n",
      "[I 2025-12-31 16:34:24,683] Trial 13 finished with value: 0.7744915058276337 and parameters: {'C': 0.04061852578249431, 'solver': 'liblinear'}. Best is trial 11 with value: 0.7757538120691798.\n",
      "[I 2025-12-31 16:34:25,536] Trial 14 finished with value: 0.7744419030338007 and parameters: {'C': 0.039057626507701095, 'solver': 'liblinear'}. Best is trial 11 with value: 0.7757538120691798.\n",
      "[I 2025-12-31 16:34:27,074] Trial 15 finished with value: 0.7705435288986213 and parameters: {'C': 0.8922218643334928, 'solver': 'liblinear'}. Best is trial 11 with value: 0.7757538120691798.\n",
      "[I 2025-12-31 16:34:27,888] Trial 16 finished with value: 0.7746409991102206 and parameters: {'C': 0.037358054085511884, 'solver': 'liblinear'}. Best is trial 11 with value: 0.7757538120691798.\n",
      "[I 2025-12-31 16:34:28,798] Trial 17 finished with value: 0.7715877128323837 and parameters: {'C': 0.08675717656962846, 'solver': 'liblinear'}. Best is trial 11 with value: 0.7757538120691798.\n",
      "[I 2025-12-31 16:34:30,332] Trial 18 finished with value: 0.7705436452730595 and parameters: {'C': 0.6591045879469322, 'solver': 'liblinear'}. Best is trial 11 with value: 0.7757538120691798.\n",
      "[I 2025-12-31 16:34:31,092] Trial 19 finished with value: 0.7765175482445134 and parameters: {'C': 0.022833147402878232, 'solver': 'liblinear'}. Best is trial 19 with value: 0.7765175482445134.\n",
      "[I 2025-12-31 16:34:31,824] Trial 20 finished with value: 0.7759527141882027 and parameters: {'C': 0.018475658260173694, 'solver': 'liblinear'}. Best is trial 19 with value: 0.7765175482445134.\n",
      "[I 2025-12-31 16:34:32,566] Trial 21 finished with value: 0.776235132975894 and parameters: {'C': 0.019612490252877292, 'solver': 'liblinear'}. Best is trial 19 with value: 0.7765175482445134.\n",
      "[I 2025-12-31 16:34:33,293] Trial 22 finished with value: 0.7763512879484393 and parameters: {'C': 0.019980601438873705, 'solver': 'liblinear'}. Best is trial 19 with value: 0.7765175482445134.\n",
      "[I 2025-12-31 16:34:34,190] Trial 23 finished with value: 0.7717532053658029 and parameters: {'C': 0.07345890578480993, 'solver': 'liblinear'}. Best is trial 19 with value: 0.7765175482445134.\n",
      "[I 2025-12-31 16:34:34,966] Trial 24 finished with value: 0.7761521843194815 and parameters: {'C': 0.019091087968864506, 'solver': 'liblinear'}. Best is trial 19 with value: 0.7765175482445134.\n",
      "[I 2025-12-31 16:34:35,820] Trial 25 finished with value: 0.7727153318183896 and parameters: {'C': 0.05585949322183197, 'solver': 'liblinear'}. Best is trial 19 with value: 0.7765175482445134.\n",
      "[I 2025-12-31 16:34:36,946] Trial 26 finished with value: 0.7704270311101582 and parameters: {'C': 0.16916826021207965, 'solver': 'liblinear'}. Best is trial 19 with value: 0.7765175482445134.\n",
      "[I 2025-12-31 16:34:37,632] Trial 27 finished with value: 0.7718534704238101 and parameters: {'C': 0.01072991140881078, 'solver': 'liblinear'}. Best is trial 19 with value: 0.7765175482445134.\n",
      "[I 2025-12-31 16:34:38,414] Trial 28 finished with value: 0.776118987829817 and parameters: {'C': 0.026665821121021917, 'solver': 'liblinear'}. Best is trial 19 with value: 0.7765175482445134.\n",
      "[I 2025-12-31 16:34:39,193] Trial 29 finished with value: 0.7761189044538561 and parameters: {'C': 0.026780549589734554, 'solver': 'liblinear'}. Best is trial 19 with value: 0.7765175482445134.\n",
      "[I 2025-12-31 16:34:40,667] A new study created in memory with name: no-name-fe8ead44-1f8b-4a35-a84e-e300bad82005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULTS FOR: Logistic Regression\n",
      "----------------------------------------\n",
      "ACCURACY     : 0.7907\n",
      "PRECISION    : 0.7003\n",
      "RECALL       : 0.7743\n",
      "F1 SCORE     : 0.7355\n",
      "K-FOLD MEAN  : 0.7862\n",
      "K-FOLD STD   : 0.0008\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "ðŸ”¥ NOW TUNING: AdaBoost\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-31 16:34:59,264] Trial 0 finished with value: 0.7938649771201436 and parameters: {'n_estimators': 258, 'learning_rate': 0.8604835707640708}. Best is trial 0 with value: 0.7938649771201436.\n",
      "[I 2025-12-31 16:35:14,151] Trial 1 finished with value: 0.3272383282895162 and parameters: {'n_estimators': 206, 'learning_rate': 0.021331842321653378}. Best is trial 0 with value: 0.7938649771201436.\n",
      "[I 2025-12-31 16:35:47,090] Trial 2 finished with value: 0.7371398704603521 and parameters: {'n_estimators': 461, 'learning_rate': 0.04022088951731519}. Best is trial 0 with value: 0.7938649771201436.\n",
      "[I 2025-12-31 16:36:10,119] Trial 3 finished with value: 0.3272383282895162 and parameters: {'n_estimators': 315, 'learning_rate': 0.01463619345761134}. Best is trial 0 with value: 0.7938649771201436.\n",
      "[I 2025-12-31 16:36:35,052] Trial 4 finished with value: 0.7858318446902016 and parameters: {'n_estimators': 345, 'learning_rate': 0.37857203901380787}. Best is trial 0 with value: 0.7938649771201436.\n",
      "[I 2025-12-31 16:37:09,792] Trial 5 finished with value: 0.7930595250563508 and parameters: {'n_estimators': 483, 'learning_rate': 0.41672952155990595}. Best is trial 0 with value: 0.7938649771201436.\n",
      "[I 2025-12-31 16:37:40,865] Trial 6 finished with value: 0.7443922895298404 and parameters: {'n_estimators': 433, 'learning_rate': 0.044979722105832225}. Best is trial 0 with value: 0.7938649771201436.\n",
      "[I 2025-12-31 16:38:01,243] Trial 7 finished with value: 0.7870215470659024 and parameters: {'n_estimators': 284, 'learning_rate': 0.5776234667397825}. Best is trial 0 with value: 0.7938649771201436.\n",
      "[I 2025-12-31 16:38:08,860] Trial 8 finished with value: 0.3272383282895162 and parameters: {'n_estimators': 102, 'learning_rate': 0.06321851604661557}. Best is trial 0 with value: 0.7938649771201436.\n",
      "[I 2025-12-31 16:38:29,765] Trial 9 finished with value: 0.5701946527752297 and parameters: {'n_estimators': 290, 'learning_rate': 0.042148076196638115}. Best is trial 0 with value: 0.7938649771201436.\n",
      "[I 2025-12-31 16:38:42,141] Trial 10 finished with value: 0.7482088005739472 and parameters: {'n_estimators': 169, 'learning_rate': 0.17002160397905366}. Best is trial 0 with value: 0.7938649771201436.\n",
      "[I 2025-12-31 16:39:10,364] Trial 11 finished with value: 0.8018620394601935 and parameters: {'n_estimators': 396, 'learning_rate': 0.9884702231375116}. Best is trial 11 with value: 0.8018620394601935.\n",
      "[I 2025-12-31 16:39:38,578] Trial 12 finished with value: 0.8024519714663504 and parameters: {'n_estimators': 393, 'learning_rate': 0.9982609618975065}. Best is trial 12 with value: 0.8024519714663504.\n",
      "[I 2025-12-31 16:40:06,430] Trial 13 finished with value: 0.7722191152012715 and parameters: {'n_estimators': 389, 'learning_rate': 0.1607284598627283}. Best is trial 12 with value: 0.8024519714663504.\n",
      "[I 2025-12-31 16:40:34,265] Trial 14 finished with value: 0.7847361513054277 and parameters: {'n_estimators': 391, 'learning_rate': 0.23205793181926082}. Best is trial 12 with value: 0.8024519714663504.\n",
      "[I 2025-12-31 16:41:01,797] Trial 15 finished with value: 0.8004287577410983 and parameters: {'n_estimators': 387, 'learning_rate': 0.8047587551223983}. Best is trial 12 with value: 0.8024519714663504.\n",
      "[I 2025-12-31 16:41:33,281] Trial 16 finished with value: 0.803770092849741 and parameters: {'n_estimators': 438, 'learning_rate': 0.9660318106754628}. Best is trial 16 with value: 0.803770092849741.\n",
      "[I 2025-12-31 16:42:09,308] Trial 17 finished with value: 0.7962662127297211 and parameters: {'n_estimators': 498, 'learning_rate': 0.31371699432243816}. Best is trial 16 with value: 0.803770092849741.\n",
      "[I 2025-12-31 16:42:40,513] Trial 18 finished with value: 0.793250531310261 and parameters: {'n_estimators': 436, 'learning_rate': 0.5199529768672064}. Best is trial 16 with value: 0.803770092849741.\n",
      "[I 2025-12-31 16:43:05,426] Trial 19 finished with value: 0.7588631051067471 and parameters: {'n_estimators': 344, 'learning_rate': 0.10018847819248923}. Best is trial 16 with value: 0.803770092849741.\n",
      "[I 2025-12-31 16:43:37,807] Trial 20 finished with value: 0.7936410473797358 and parameters: {'n_estimators': 453, 'learning_rate': 0.6261083423180152}. Best is trial 16 with value: 0.803770092849741.\n",
      "[I 2025-12-31 16:44:07,242] Trial 21 finished with value: 0.7968692498668166 and parameters: {'n_estimators': 409, 'learning_rate': 0.8332434082980994}. Best is trial 16 with value: 0.803770092849741.\n",
      "[I 2025-12-31 16:44:33,570] Trial 22 finished with value: 0.8100320508234157 and parameters: {'n_estimators': 366, 'learning_rate': 0.9793201945455869}. Best is trial 22 with value: 0.8100320508234157.\n",
      "[I 2025-12-31 16:44:58,753] Trial 23 finished with value: 0.783539620845635 and parameters: {'n_estimators': 351, 'learning_rate': 0.2785843306791896}. Best is trial 22 with value: 0.8100320508234157.\n",
      "[I 2025-12-31 16:45:24,902] Trial 24 finished with value: 0.7843230448997239 and parameters: {'n_estimators': 361, 'learning_rate': 0.5295806110542534}. Best is trial 22 with value: 0.8100320508234157.\n",
      "[I 2025-12-31 16:45:55,438] Trial 25 finished with value: 0.7911402326963217 and parameters: {'n_estimators': 423, 'learning_rate': 0.6211372604588082}. Best is trial 22 with value: 0.8100320508234157.\n",
      "[I 2025-12-31 16:46:18,072] Trial 26 finished with value: 0.7732666550715487 and parameters: {'n_estimators': 316, 'learning_rate': 0.2020428112304696}. Best is trial 22 with value: 0.8100320508234157.\n",
      "[I 2025-12-31 16:46:35,726] Trial 27 finished with value: 0.7534801875207483 and parameters: {'n_estimators': 247, 'learning_rate': 0.11727237053889712}. Best is trial 22 with value: 0.8100320508234157.\n",
      "[I 2025-12-31 16:47:09,116] Trial 28 finished with value: 0.7872331851055901 and parameters: {'n_estimators': 470, 'learning_rate': 0.3515486673005359}. Best is trial 22 with value: 0.8100320508234157.\n",
      "[I 2025-12-31 16:47:35,059] Trial 29 finished with value: 0.7966058147835847 and parameters: {'n_estimators': 362, 'learning_rate': 0.9688346879715202}. Best is trial 22 with value: 0.8100320508234157.\n",
      "[I 2025-12-31 16:48:39,162] A new study created in memory with name: no-name-00d2d5a4-afe0-4ee5-8109-96756f49d783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULTS FOR: AdaBoost\n",
      "----------------------------------------\n",
      "ACCURACY     : 0.8282\n",
      "PRECISION    : 0.7582\n",
      "RECALL       : 0.7968\n",
      "F1 SCORE     : 0.7770\n",
      "K-FOLD MEAN  : 0.8275\n",
      "K-FOLD STD   : 0.0017\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "ðŸ”¥ NOW TUNING: DummyClassifier\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-31 16:48:39,488] Trial 0 finished with value: 0.5004871478847565 and parameters: {'strategy': 'uniform'}. Best is trial 0 with value: 0.5004871478847565.\n",
      "[I 2025-12-31 16:48:39,808] Trial 1 finished with value: 0.4989967052603787 and parameters: {'strategy': 'uniform'}. Best is trial 0 with value: 0.5004871478847565.\n",
      "[I 2025-12-31 16:48:40,127] Trial 2 finished with value: 0.4989146193285861 and parameters: {'strategy': 'stratified'}. Best is trial 0 with value: 0.5004871478847565.\n",
      "[I 2025-12-31 16:48:40,445] Trial 3 finished with value: 0.6666666666666666 and parameters: {'strategy': 'most_frequent'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "[I 2025-12-31 16:48:40,766] Trial 4 finished with value: 0.6666666666666666 and parameters: {'strategy': 'most_frequent'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "[I 2025-12-31 16:48:41,095] Trial 5 finished with value: 0.6666666666666666 and parameters: {'strategy': 'most_frequent'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "[I 2025-12-31 16:48:41,418] Trial 6 finished with value: 0.4990460103371696 and parameters: {'strategy': 'uniform'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "[I 2025-12-31 16:48:41,734] Trial 7 finished with value: 0.4995434486286902 and parameters: {'strategy': 'stratified'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "[I 2025-12-31 16:48:42,052] Trial 8 finished with value: 0.6666666666666666 and parameters: {'strategy': 'most_frequent'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "[I 2025-12-31 16:48:42,369] Trial 9 finished with value: 0.49637639331148575 and parameters: {'strategy': 'stratified'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "[I 2025-12-31 16:48:42,691] Trial 10 finished with value: 0.6666666666666666 and parameters: {'strategy': 'most_frequent'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "[I 2025-12-31 16:48:43,003] Trial 11 finished with value: 0.6666666666666666 and parameters: {'strategy': 'most_frequent'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "[I 2025-12-31 16:48:43,324] Trial 12 finished with value: 0.6666666666666666 and parameters: {'strategy': 'most_frequent'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "[I 2025-12-31 16:48:43,642] Trial 13 finished with value: 0.6666666666666666 and parameters: {'strategy': 'most_frequent'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "[I 2025-12-31 16:48:43,961] Trial 14 finished with value: 0.6666666666666666 and parameters: {'strategy': 'most_frequent'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "[I 2025-12-31 16:48:44,283] Trial 15 finished with value: 0.6666666666666666 and parameters: {'strategy': 'most_frequent'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "[I 2025-12-31 16:48:44,599] Trial 16 finished with value: 0.6666666666666666 and parameters: {'strategy': 'most_frequent'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "[I 2025-12-31 16:48:44,918] Trial 17 finished with value: 0.6666666666666666 and parameters: {'strategy': 'most_frequent'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "[I 2025-12-31 16:48:45,236] Trial 18 finished with value: 0.4953814197089022 and parameters: {'strategy': 'stratified'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "[I 2025-12-31 16:48:45,563] Trial 19 finished with value: 0.5018033835907351 and parameters: {'strategy': 'uniform'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "[I 2025-12-31 16:48:45,888] Trial 20 finished with value: 0.6666666666666666 and parameters: {'strategy': 'most_frequent'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "[I 2025-12-31 16:48:46,210] Trial 21 finished with value: 0.6666666666666666 and parameters: {'strategy': 'most_frequent'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "[I 2025-12-31 16:48:46,533] Trial 22 finished with value: 0.6666666666666666 and parameters: {'strategy': 'most_frequent'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "[I 2025-12-31 16:48:46,851] Trial 23 finished with value: 0.6666666666666666 and parameters: {'strategy': 'most_frequent'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "[I 2025-12-31 16:48:47,170] Trial 24 finished with value: 0.6666666666666666 and parameters: {'strategy': 'most_frequent'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "[I 2025-12-31 16:48:47,491] Trial 25 finished with value: 0.6666666666666666 and parameters: {'strategy': 'most_frequent'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "[I 2025-12-31 16:48:47,807] Trial 26 finished with value: 0.6666666666666666 and parameters: {'strategy': 'most_frequent'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "[I 2025-12-31 16:48:48,123] Trial 27 finished with value: 0.6666666666666666 and parameters: {'strategy': 'most_frequent'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "[I 2025-12-31 16:48:48,443] Trial 28 finished with value: 0.5005769628493686 and parameters: {'strategy': 'stratified'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "[I 2025-12-31 16:48:48,760] Trial 29 finished with value: 0.5010731477131511 and parameters: {'strategy': 'uniform'}. Best is trial 3 with value: 0.6666666666666666.\n",
      "C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š RESULTS FOR: DummyClassifier\n",
      "----------------------------------------\n",
      "ACCURACY     : 0.6243\n",
      "PRECISION    : 0.0000\n",
      "RECALL       : 0.0000\n",
      "F1 SCORE     : 0.0000\n",
      "K-FOLD MEAN  : 0.3331\n",
      "K-FOLD STD   : 0.0001\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    'SGDC', 'Bagging', 'Stacking', 'LGBMClassifier', 'XGBoost', 'CatBoost',\n",
    "    'HistGradientBoosting', 'MLPC', 'Hard Voting', 'RidgeClassifier',\n",
    "    'Soft Voting', 'RandomForest', 'DecisionTree', 'GradientBoosting',\n",
    "    'Bagged DT', 'SVC', 'KNN', 'ExtraTrees', 'Passive Aggressive',\n",
    "    'Logistic Regression', 'AdaBoost', 'DummyClassifier'\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for name in models:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"ðŸ”¥ NOW TUNING: {name}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, name), n_trials=30)\n",
    "\n",
    "    best_model = get_model(study.best_trial, name)\n",
    "    best_model.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = best_model.predict(x_test)\n",
    "\n",
    "    # ===== Test metrics =====\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # ===== K-Fold metrics =====\n",
    "    kf_scores = cross_val_score(\n",
    "        best_model,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        cv=kf,\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    kf_mean = kf_scores.mean()\n",
    "    kf_std = kf_scores.std()\n",
    "\n",
    "    # ===== PRINT RESULTS =====\n",
    "    print(f\"\\nðŸ“Š RESULTS FOR: {name}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"ACCURACY     : {acc:.4f}\")\n",
    "    print(f\"PRECISION    : {prec:.4f}\")\n",
    "    print(f\"RECALL       : {rec:.4f}\")\n",
    "    print(f\"F1 SCORE     : {f1:.4f}\")\n",
    "    print(f\"K-FOLD MEAN  : {kf_mean:.4f}\")\n",
    "    print(f\"K-FOLD STD   : {kf_std:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    results.append([\n",
    "        name,\n",
    "        acc,\n",
    "        prec,\n",
    "        rec,\n",
    "        f1,\n",
    "        kf_mean,\n",
    "        kf_std\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b136f127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                            Optuna                                            </span>\n",
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Algorithm            </span>â”ƒ<span style=\"font-weight: bold\"> Accuracy </span>â”ƒ<span style=\"font-weight: bold\"> Precision </span>â”ƒ<span style=\"font-weight: bold\"> Recall </span>â”ƒ<span style=\"font-weight: bold\"> F1-score </span>â”ƒ<span style=\"font-weight: bold\"> K-Fold mean </span>â”ƒ<span style=\"font-weight: bold\"> K-Fold std </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">SGDC</span>                 â”‚ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.38</span>     â”‚ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.38</span>      â”‚ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.00</span>   â”‚ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.55</span>     â”‚ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.33</span>        â”‚ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.00</span>       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Stacking             â”‚ 0.89     â”‚ 0.86      â”‚ 0.83   â”‚ 0.85     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Bagged DT            â”‚ 0.88     â”‚ 0.85      â”‚ 0.83   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Bagging              â”‚ 0.89     â”‚ 0.86      â”‚ 0.83   â”‚ 0.85     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ LGBMClassifier       â”‚ 0.88     â”‚ 0.86      â”‚ 0.83   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ GradientBoosting     â”‚ 0.88     â”‚ 0.85      â”‚ 0.82   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ HistGradientBoosting â”‚ 0.87     â”‚ 0.84      â”‚ 0.82   â”‚ 0.83     â”‚ 0.89        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Soft Voting          â”‚ 0.88     â”‚ 0.86      â”‚ 0.82   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ CatBoost             â”‚ 0.88     â”‚ 0.86      â”‚ 0.82   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ XGBoost              â”‚ 0.88     â”‚ 0.86      â”‚ 0.82   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Hard Voting          â”‚ 0.88     â”‚ 0.85      â”‚ 0.82   â”‚ 0.83     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ RandomForest         â”‚ 0.88     â”‚ 0.87      â”‚ 0.82   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ ExtraTrees           â”‚ 0.87     â”‚ 0.84      â”‚ 0.81   â”‚ 0.83     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ SVC                  â”‚ 0.84     â”‚ 0.77      â”‚ 0.81   â”‚ 0.79     â”‚ 0.84        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ DecisionTree         â”‚ 0.85     â”‚ 0.81      â”‚ 0.80   â”‚ 0.80     â”‚ 0.86        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ AdaBoost             â”‚ 0.83     â”‚ 0.76      â”‚ 0.80   â”‚ 0.78     â”‚ 0.83        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Passive Aggressive   â”‚ 0.76     â”‚ 0.65      â”‚ 0.79   â”‚ 0.71     â”‚ 0.73        â”‚ 0.02       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ KNN                  â”‚ 0.78     â”‚ 0.68      â”‚ 0.79   â”‚ 0.73     â”‚ 0.83        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ MLPC                 â”‚ 0.84     â”‚ 0.78      â”‚ 0.78   â”‚ 0.78     â”‚ 0.85        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Logistic Regression  â”‚ 0.79     â”‚ 0.70      â”‚ 0.77   â”‚ 0.74     â”‚ 0.79        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ RidgeClassifier      â”‚ 0.80     â”‚ 0.71      â”‚ 0.77   â”‚ 0.74     â”‚ 0.79        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">DummyClassifier</span>      â”‚ <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.62</span>     â”‚ <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.00</span>      â”‚ <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.00</span>   â”‚ <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.00</span>     â”‚ <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.33</span>        â”‚ <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.00</span>       â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                            Optuna                                            \u001b[0m\n",
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mAlgorithm           \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mAccuracy\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mPrecision\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mRecall\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mF1-score\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mK-Fold mean\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mK-Fold std\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ \u001b[1;32mSGDC\u001b[0m                 â”‚ \u001b[1;32m0.38\u001b[0m     â”‚ \u001b[1;32m0.38\u001b[0m      â”‚ \u001b[1;32m1.00\u001b[0m   â”‚ \u001b[1;32m0.55\u001b[0m     â”‚ \u001b[1;32m0.33\u001b[0m        â”‚ \u001b[1;32m0.00\u001b[0m       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Stacking             â”‚ 0.89     â”‚ 0.86      â”‚ 0.83   â”‚ 0.85     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Bagged DT            â”‚ 0.88     â”‚ 0.85      â”‚ 0.83   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Bagging              â”‚ 0.89     â”‚ 0.86      â”‚ 0.83   â”‚ 0.85     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ LGBMClassifier       â”‚ 0.88     â”‚ 0.86      â”‚ 0.83   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ GradientBoosting     â”‚ 0.88     â”‚ 0.85      â”‚ 0.82   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ HistGradientBoosting â”‚ 0.87     â”‚ 0.84      â”‚ 0.82   â”‚ 0.83     â”‚ 0.89        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Soft Voting          â”‚ 0.88     â”‚ 0.86      â”‚ 0.82   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ CatBoost             â”‚ 0.88     â”‚ 0.86      â”‚ 0.82   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ XGBoost              â”‚ 0.88     â”‚ 0.86      â”‚ 0.82   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Hard Voting          â”‚ 0.88     â”‚ 0.85      â”‚ 0.82   â”‚ 0.83     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ RandomForest         â”‚ 0.88     â”‚ 0.87      â”‚ 0.82   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ ExtraTrees           â”‚ 0.87     â”‚ 0.84      â”‚ 0.81   â”‚ 0.83     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ SVC                  â”‚ 0.84     â”‚ 0.77      â”‚ 0.81   â”‚ 0.79     â”‚ 0.84        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ DecisionTree         â”‚ 0.85     â”‚ 0.81      â”‚ 0.80   â”‚ 0.80     â”‚ 0.86        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ AdaBoost             â”‚ 0.83     â”‚ 0.76      â”‚ 0.80   â”‚ 0.78     â”‚ 0.83        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Passive Aggressive   â”‚ 0.76     â”‚ 0.65      â”‚ 0.79   â”‚ 0.71     â”‚ 0.73        â”‚ 0.02       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ KNN                  â”‚ 0.78     â”‚ 0.68      â”‚ 0.79   â”‚ 0.73     â”‚ 0.83        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ MLPC                 â”‚ 0.84     â”‚ 0.78      â”‚ 0.78   â”‚ 0.78     â”‚ 0.85        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Logistic Regression  â”‚ 0.79     â”‚ 0.70      â”‚ 0.77   â”‚ 0.74     â”‚ 0.79        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ RidgeClassifier      â”‚ 0.80     â”‚ 0.71      â”‚ 0.77   â”‚ 0.74     â”‚ 0.79        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ \u001b[1;31mDummyClassifier\u001b[0m      â”‚ \u001b[1;31m0.62\u001b[0m     â”‚ \u001b[1;31m0.00\u001b[0m      â”‚ \u001b[1;31m0.00\u001b[0m   â”‚ \u001b[1;31m0.00\u001b[0m     â”‚ \u001b[1;31m0.33\u001b[0m        â”‚ \u001b[1;31m0.00\u001b[0m       â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console = Console()\n",
    "\n",
    "result_sorted = sorted(results, key=lambda i: i[3], reverse=True)\n",
    "\n",
    "best_model = max(results, key=lambda x: x[3])\n",
    "worst_model = min(results, key=lambda x: x[3])\n",
    "\n",
    "table = Table(title=\"Optuna\", show_lines=True)\n",
    "table.add_column(\"Algorithm\")\n",
    "table.add_column(\"Accuracy\")\n",
    "table.add_column(\"Precision\")\n",
    "table.add_column(\"Recall\")\n",
    "table.add_column(\"F1-score\")\n",
    "table.add_column(\"K-Fold mean\")\n",
    "table.add_column(\"K-Fold std\")\n",
    "\n",
    "for row in result_sorted:\n",
    "    algo, acc, presicion, recall, f1, kmean, kstd = row\n",
    "\n",
    "    if row == best_model:\n",
    "        table.add_row(\n",
    "            f\"[bold green]{algo}[/bold green]\",\n",
    "            f\"[bold green]{acc:.2f}[/bold green]\",\n",
    "            f\"[bold green]{presicion:.2f}[/bold green]\",\n",
    "            f\"[bold green]{recall:.2f}[/bold green]\",\n",
    "            f\"[bold green]{f1:.2f}[/bold green]\",\n",
    "            f\"[bold green]{kmean:.2f}[/bold green]\",\n",
    "            f\"[bold green]{kstd:.2f}[/bold green]\",\n",
    "        )\n",
    "    elif row == worst_model:\n",
    "        table.add_row(\n",
    "            f\"[bold red]{algo}[/bold red]\",\n",
    "            f\"[bold red]{acc:.2f}[/bold red]\",\n",
    "            f\"[bold red]{presicion:.2f}[/bold red]\",\n",
    "            f\"[bold red]{recall:.2f}[/bold red]\",\n",
    "            f\"[bold red]{f1:.2f}[/bold red]\",\n",
    "            f\"[bold red]{kmean:.2f}[/bold red]\",\n",
    "            f\"[bold red]{kstd:.2f}[/bold red]\",\n",
    "        )\n",
    "    else:\n",
    "        table.add_row(algo, f\"{acc:.2f}\", f\"{presicion:.2f}\", f\"{recall:.2f}\", f\"{f1:.2f}\", f\"{kmean:.2f}\", f\"{kstd:.2f}\")\n",
    "\n",
    "console.print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8e42bbeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                            Optuna                                            </span>\n",
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Algorithm            </span>â”ƒ<span style=\"font-weight: bold\"> Accuracy </span>â”ƒ<span style=\"font-weight: bold\"> Precision </span>â”ƒ<span style=\"font-weight: bold\"> Recall </span>â”ƒ<span style=\"font-weight: bold\"> F1-score </span>â”ƒ<span style=\"font-weight: bold\"> K-Fold mean </span>â”ƒ<span style=\"font-weight: bold\"> K-Fold std </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">SGDC</span>                 â”‚ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.38</span>     â”‚ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.38</span>      â”‚ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.00</span>   â”‚ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.55</span>     â”‚ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.33</span>        â”‚ <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.00</span>       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Stacking             â”‚ 0.89     â”‚ 0.86      â”‚ 0.83   â”‚ 0.85     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Bagged DT            â”‚ 0.88     â”‚ 0.85      â”‚ 0.83   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Bagging              â”‚ 0.89     â”‚ 0.86      â”‚ 0.83   â”‚ 0.85     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ LGBMClassifier       â”‚ 0.88     â”‚ 0.86      â”‚ 0.83   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ GradientBoosting     â”‚ 0.88     â”‚ 0.85      â”‚ 0.82   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ HistGradientBoosting â”‚ 0.87     â”‚ 0.84      â”‚ 0.82   â”‚ 0.83     â”‚ 0.89        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Soft Voting          â”‚ 0.88     â”‚ 0.86      â”‚ 0.82   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ CatBoost             â”‚ 0.88     â”‚ 0.86      â”‚ 0.82   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ XGBoost              â”‚ 0.88     â”‚ 0.86      â”‚ 0.82   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Hard Voting          â”‚ 0.88     â”‚ 0.85      â”‚ 0.82   â”‚ 0.83     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ RandomForest         â”‚ 0.88     â”‚ 0.87      â”‚ 0.82   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ ExtraTrees           â”‚ 0.87     â”‚ 0.84      â”‚ 0.81   â”‚ 0.83     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ SVC                  â”‚ 0.84     â”‚ 0.77      â”‚ 0.81   â”‚ 0.79     â”‚ 0.84        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ DecisionTree         â”‚ 0.85     â”‚ 0.81      â”‚ 0.80   â”‚ 0.80     â”‚ 0.86        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ AdaBoost             â”‚ 0.83     â”‚ 0.76      â”‚ 0.80   â”‚ 0.78     â”‚ 0.83        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Passive Aggressive   â”‚ 0.76     â”‚ 0.65      â”‚ 0.79   â”‚ 0.71     â”‚ 0.73        â”‚ 0.02       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ KNN                  â”‚ 0.78     â”‚ 0.68      â”‚ 0.79   â”‚ 0.73     â”‚ 0.83        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ MLPC                 â”‚ 0.84     â”‚ 0.78      â”‚ 0.78   â”‚ 0.78     â”‚ 0.85        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Logistic Regression  â”‚ 0.79     â”‚ 0.70      â”‚ 0.77   â”‚ 0.74     â”‚ 0.79        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ RidgeClassifier      â”‚ 0.80     â”‚ 0.71      â”‚ 0.77   â”‚ 0.74     â”‚ 0.79        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">DummyClassifier</span>      â”‚ <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.62</span>     â”‚ <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.00</span>      â”‚ <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.00</span>   â”‚ <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.00</span>     â”‚ <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.33</span>        â”‚ <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0.00</span>       â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                            Optuna                                            \u001b[0m\n",
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mAlgorithm           \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mAccuracy\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mPrecision\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mRecall\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mF1-score\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mK-Fold mean\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mK-Fold std\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ \u001b[1;32mSGDC\u001b[0m                 â”‚ \u001b[1;32m0.38\u001b[0m     â”‚ \u001b[1;32m0.38\u001b[0m      â”‚ \u001b[1;32m1.00\u001b[0m   â”‚ \u001b[1;32m0.55\u001b[0m     â”‚ \u001b[1;32m0.33\u001b[0m        â”‚ \u001b[1;32m0.00\u001b[0m       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Stacking             â”‚ 0.89     â”‚ 0.86      â”‚ 0.83   â”‚ 0.85     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Bagged DT            â”‚ 0.88     â”‚ 0.85      â”‚ 0.83   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Bagging              â”‚ 0.89     â”‚ 0.86      â”‚ 0.83   â”‚ 0.85     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ LGBMClassifier       â”‚ 0.88     â”‚ 0.86      â”‚ 0.83   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ GradientBoosting     â”‚ 0.88     â”‚ 0.85      â”‚ 0.82   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ HistGradientBoosting â”‚ 0.87     â”‚ 0.84      â”‚ 0.82   â”‚ 0.83     â”‚ 0.89        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Soft Voting          â”‚ 0.88     â”‚ 0.86      â”‚ 0.82   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ CatBoost             â”‚ 0.88     â”‚ 0.86      â”‚ 0.82   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ XGBoost              â”‚ 0.88     â”‚ 0.86      â”‚ 0.82   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Hard Voting          â”‚ 0.88     â”‚ 0.85      â”‚ 0.82   â”‚ 0.83     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ RandomForest         â”‚ 0.88     â”‚ 0.87      â”‚ 0.82   â”‚ 0.84     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ ExtraTrees           â”‚ 0.87     â”‚ 0.84      â”‚ 0.81   â”‚ 0.83     â”‚ 0.90        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ SVC                  â”‚ 0.84     â”‚ 0.77      â”‚ 0.81   â”‚ 0.79     â”‚ 0.84        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ DecisionTree         â”‚ 0.85     â”‚ 0.81      â”‚ 0.80   â”‚ 0.80     â”‚ 0.86        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ AdaBoost             â”‚ 0.83     â”‚ 0.76      â”‚ 0.80   â”‚ 0.78     â”‚ 0.83        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Passive Aggressive   â”‚ 0.76     â”‚ 0.65      â”‚ 0.79   â”‚ 0.71     â”‚ 0.73        â”‚ 0.02       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ KNN                  â”‚ 0.78     â”‚ 0.68      â”‚ 0.79   â”‚ 0.73     â”‚ 0.83        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ MLPC                 â”‚ 0.84     â”‚ 0.78      â”‚ 0.78   â”‚ 0.78     â”‚ 0.85        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Logistic Regression  â”‚ 0.79     â”‚ 0.70      â”‚ 0.77   â”‚ 0.74     â”‚ 0.79        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ RidgeClassifier      â”‚ 0.80     â”‚ 0.71      â”‚ 0.77   â”‚ 0.74     â”‚ 0.79        â”‚ 0.00       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ \u001b[1;31mDummyClassifier\u001b[0m      â”‚ \u001b[1;31m0.62\u001b[0m     â”‚ \u001b[1;31m0.00\u001b[0m      â”‚ \u001b[1;31m0.00\u001b[0m   â”‚ \u001b[1;31m0.00\u001b[0m     â”‚ \u001b[1;31m0.33\u001b[0m        â”‚ \u001b[1;31m0.00\u001b[0m       â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_console = Console(record=True)\n",
    "temp_console.print(table)\n",
    "text = temp_console.export_text()\n",
    "with open('results/Tuning.txt', 'a', encoding='utf-8') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d84bf7",
   "metadata": {},
   "source": [
    "# Another Tunings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802ca834",
   "metadata": {},
   "source": [
    "<a href=\"../Tuning/manual_search.ipynb\" target=\"_blank\" style=\"text-decoration: none;\">\n",
    "    <button style=\"\n",
    "        background-color: #45a049;\n",
    "        color: white; \n",
    "        padding: 12px 24px; \n",
    "        font-size: 16px; \n",
    "        border: none; \n",
    "        border-radius: 8px;\n",
    "        cursor: pointer;\n",
    "        transition: background-color 0.3s;\n",
    "    \" onmouseover=\"this.style.backgroundColor='#45a049'\" onmouseout=\"this.style.backgroundColor='#4CAF50'\">\n",
    "        Manual Search\n",
    "    </button>\n",
    "</a>\n",
    "\n",
    "<a href=\"../Tuning/optuna.ipynb\" target=\"_blank\" style=\"text-decoration: none;\">\n",
    "    <button style=\"\n",
    "        background-color: #45a049;\n",
    "        color: white; \n",
    "        padding: 12px 24px; \n",
    "        font-size: 16px; \n",
    "        border: none; \n",
    "        border-radius: 8px;\n",
    "        cursor: pointer;\n",
    "        transition: background-color 0.3s;\n",
    "    \" onmouseover=\"this.style.backgroundColor='#45a049'\" onmouseout=\"this.style.backgroundColor='#4CAF50'\">\n",
    "        optuna\n",
    "    </button>\n",
    "</a>\n",
    "\n",
    "<a href=\"../Tuning/random_search.ipynb\" target=\"_blank\" style=\"text-decoration: none;\">\n",
    "    <button style=\"\n",
    "        background-color: #45a049;\n",
    "        color: white; \n",
    "        padding: 12px 24px; \n",
    "        font-size: 16px; \n",
    "        border: none; \n",
    "        border-radius: 8px;\n",
    "        cursor: pointer;\n",
    "        transition: background-color 0.3s;\n",
    "    \" onmouseover=\"this.style.backgroundColor='#45a049'\" onmouseout=\"this.style.backgroundColor='#4CAF50'\">\n",
    "        Random Search\n",
    "    </button>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21380bf5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
